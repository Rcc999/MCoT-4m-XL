

**MINT: Multi-modal Chain of Thought**


**in Unified Generative Models for Enhanced Image Generation**


**Yi Wang** 1 2 \***Mushui Liu** 1 2 \***Wanggui He** 2 \* *†***Longxiang Zhang** 2 +**Ziwei Huang** 1 2 +**Guanghao Zhang** 2 +


**Fangxun Shu** 2**ZhongTao** 2**Dong She** 2**Zhelun Yu** 2**Haoyuan Li** 2**Weilong Dai** 2**Mingli Song** 1


**Jie Song** 1 *‡***Hao Jiang** 2 *‡*


1**Zhejiang University**


2**Alibaba Group**


\*Equal contribution


*†*Project lead


+Core contributor


*‡*Corresponding author


*{*y w, lms, 22351096, sjie, brooksong*}*@zju.edu.cn


*{*wanggui.hwg, shengxiang.zlx, guanghao.zgh, junmu.dwl, aoshu.jh*}*@taobao.com




Landscape



Portrait


Architecture





Indoor


Plant


Animal


Food



Design



Landscape







Color



Co-occurrence


Position









Count


Mixed attribute




**(a)**


**(b)**


Figure 1: Visualization of images generated by MINT. (a) Images with various themes generated by MINT demonstrate


its exceptional versatility while exhibiting extraordinary detail quality. (b) Integrated with Multimodal Chain of Thought


(MCoT), MINT exhibits remarkable capabilities in understanding and reasoning about text-image relationships, effectively


adhering to the interwoven conditions present in intricate imagery.


1


arXiv:2503.01298v1 [cs.CV] 3 Mar 2025




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


**Abstract**


Unified generative models have demonstrated ex-


traordinary performance in both text and image


generation. However, they tend to underperform


when generating intricate images with various in-


terwoven conditions, which is hard to solely rely


on straightforward text-to-image generation. In re-


sponse to this challenge, we introduce ***MINT***, an


innovative unified generative model, empowered


with native multimodal chain of thought (MCoT)


for enhanced image generation for the first time.


Firstly, we design Mixture of Transformer Ex-


perts (MTXpert), an expert-parallel structure that


effectively supports both natural language genera-


tion (NLG) and visual capabilities, while avoid-


ing potential modality conflicts that could hin-


der the full potential of each modality. Building


on this, we propose an innovative MCoT train-


ing paradigm—a step-by-step approach to multi-


modal thinking, reasoning, and reflection specifi-


cally designed to enhance image generation. This


paradigm equips MINT with nuanced, element-


wise decoupled alignment and a comprehensive


understanding of textual and visual components.


Furthermore, it fosters advanced multimodal rea-


soning and self-reflection, enabling the construc-


tion of images that are firmly grounded in the


logical relationships between these elements. No-


tably, MINT has been validated to exhibit superior


performance across multiple benchmarks for text-


to-image (T2I) and image-to-text (I2T) tasks.


**1. Introduction**


Unified generative models (Kondratyuk et al., 2023; He


et al., 2024; Bachmann et al., 2024; Zhou et al., 2024b) have


recently demonstrated superior capabilities in understanding


and generating multimodal information, *e.g.*, linguistic and


visual data. Prior works (Ding et al., 2021; Team, 2024b;


Lu et al., 2024; Kondratyuk et al., 2023) employ next-token


prediction frameworks that represent all modalities as dis-


crete tokens to capture cross-modal interactions. However,


such discrete tokenization approaches do not align with the


continuous nature of images and videos, thereby limiting


the generative potential.


To address these limitations, recent studies (Zhou et al.,


2024b; Xiao et al., 2024; Ma et al., 2024) have explored


hybrid generative models that process images as continuous


data using diffusion manner and text as discrete data through


an autoregressive manner. These models can achieve com-


parable and even superior performance to diffusion mod-


els in image generation tasks. However, as illustrated in


Figure 2, when generating intricate images—particularly


**Attribute Confusion**


**Object Defect**


**Spatial Confusion**




**Concept Confusion**








**(a)**


**(b)**


a baseball bat and a fork


a blue clock and a white cup


a scissors and a bowl


a book above a laptop


Figure 2: Limitations of straightforward text-to-image (T2I)


generation. (a) T2I generation results illustrate confusion


and defects. (b) The results generated by MINT with MCoT


effectively address these issues.


those with various interwoven conditions, a straightforward


text-to-image (T2I) approach often falls short of meeting


users’ expectations in a single attempt. This necessitates


models with exceptionally strong fine-grained image-text


alignment, concept-wise understanding, and reasoning ca-


pabilities, which is precisely what we anticipate unified


generative models will ultimately achieve, due to their inher-


ent advantage of multimodal information interaction within


a single model. Therefore, a natural question arises: *Is it fea-*


*sible to harness the multimodal understanding, reasoning,*


*and generative capabilities of unified generative models to*


*effectively address such intricate and realistic multimodal*


*challenges in a step-by-step manner?*


In this paper, we introduce ***MINT***, an innovative unified gen-


erative model, empowered by ***multimodal chain of thought***


***(MCoT)*** for enhanced image generation. MCoT represents


step-by-step multimodal thinking and reasoning, akin to the


chain of thought (CoT) (Wei et al., 2022b), thereby further


unlocking the multimodal potential of unified generative


models. To achieve this, as shown in Figure 3, we firstly de-


sign the ***Mixture of Transformer Experts (MTXpert)*** as the


core of MINT, which preserves the natural language process-


ing capabilities of pretrained large language models while


seamlessly endowing the model with exceptional visual un-


derstanding and generative abilities. Moreover, MTXpert,


an expert-parallel design, avoids potential conflicts between


different modalities that can occur in a single Transformer


model (Liang et al., 2024a; Shi et al., 2024), thereby fa-


cilitating a comprehensive interplay and deep alignment


between the textual and visual modalities, which is vital for


unlocking multimodal thinking and reasoning.


Finally, and most importantly, to activate the inherent MCoT


skill set of MINT, we propose an innovative MCOT train-


ing paradigm, as illustrated in Figure 3. Generating in-


tricate images—such as those featuring multiple objects,


complex spatial relationships, and various attributes that


are all interwoven—requires nuanced, element-wise decou-


2




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**



**<Caption Planning>**


The image depicts a train station with a blue and white train


on the tracks. The train is positioned on the left side of the


image, and the platform is visible in the foreground. The


platform has a yellow safety line. The station appears to be


outdoors, with trees and utility poles in the background. The


sky is overcast, and the overall scene suggests a typical day


at a train station.


Text Tokenizer


The cutest shiba inu sitting on


the ground with fallen leaves.


VAE


VAE


Text


Image for 


Understanding


Image for 


Generation


<img>


</img> <img>


</img>


<img>


</img> <img>


**...**


**Next Token Prediction**


**Noise Prediction**




Router



LayerNorm



LayerNorm



LayerNorm



QKV



QKV



QKV


**Linguistic**


**Expert**


**Semantic**


**Visual Expert**


**Generative**


**Visual Expert**



**Multimodal Attention**


**Attention**




LayerNorm



LayerNorm



LayerNorm



FFN



FFN



FFN



Router


**FFN**



**Mixture of Transformer Experts (MTXpert)**


Noise











**Multimodal Chain of Thought for Image Generation**


②**ACTING**


③**REFLECTION**


④**CORRECTION**


(a)


(b)



**<Layout Planning>**


[{"a train.": "0.002,0.002,0.682,0.992"}, ...... ]


**<User Input>**


Generate image with caption: An AI Generated Image


a photo of a train


Plan then generate: give dense caption and layout then draw




**<Artifact Map>**


**<Artifact Reflection>**


Detect artifact in image: 


caption: 


a photo of a train


image: 


<|img\_start|> ... <|img\_end|>



**<Artifact Correction>**


Inpainting image with mask: 


caption and layout:


{"densecaption": "...", "bbox": ... }


masked image:


<|img\_start|> ... <|img\_end|>


①**PLANNING**


Figure 3: Overall Framework of the Proposed MINT. (a) The illustration of MINT architecture, highlighting expert-parallel


MTXpert as the core component. (b) Multimodal Chain of Thought (MCoT) empowers MINT with nuanced, element-wise


decoupled alignment, facilitating advanced multimodal thinking and reasoning capabilities.


pled alignment and a deep understanding of textual and


visual components. This process further entails the ability


to fully comprehend the semantic connections among these


elements and their corresponding text, thereby enabling the


construction of images that are firmly grounded in the logi-


cal relationships between them. However, straightforward


T2I generation is insufficient to effectively address these


challenges. To overcome these limitations, we introduce the


MCoT training paradigm to equip MINT with native MCoT


capabilities. MCoT replaces the straightforward T2I pro-


cess with step-by-step multimodal thinking, reasoning, and


reflection, thereby alleviating the difficulties associated with


generating intricate images. All MCoT steps are handled en-


tirely within MINT as shown in Figure 3, where the planning


and acting steps explicitly enhance fine-grained alignment


between text and images, as well as the decoupling of visual


elements. Furthermore, the reflection and correction steps


enable the model to self-assess artifacts within the generated


image and identify discrepancies between the image and the


corresponding caption, which endows MINT with human-


like evaluative capabilities, further refining image details


and ultimately improving the quality of image generation.


In general, our contributions can be summarized as follows:


• We introduce MINT, a unified generative model that


seamlessly integrates understanding and generation


capabilities of both textual and visual data. MINT de-


couples the model parameters associated with different


tasks, specifically text generation, image understand-


ing, and image generation, including feed-forward net-


works, attention matrices, and layer normalization. Fur-


ther, it facilitates task-specific processing using global


self-attention over the entire input sequence, result-


ing in superior performance in multimodal tasks and


demonstrating the potential for expansion to any-to-any


tasks.


• We propose the MCoT training paradigm to activate the


native MCoT capabilities of unified generative models


for the first time, thereby revealing previously untapped


potential within large-scale text-image pre-training and


demonstrating its effectiveness in enhancing perfor-


3




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


mance for intricate and realistic multimodal challenges,


such as intricate image generation.


• MINT demonstrates an enhanced ability in multimodal


thinking and reasoning, enabling it to effectively break


down complex problems and achieve exceptional per-


formance, which is validated across a series of mul-


timodal benchmarks, including GenEval, MS-COCO,


VQA-v2, MME, and MMBench.


**2. Related Works**


**Generative Foundation Models.** The GPT series (OpenAI,


2023) have demonstrated that language models can learn


numerous tasks via training on a large-scale dataset. Beyond


language, multi-modal large language models (Liu et al.,


2023; Chen et al., 2024) have been proposed to integrate


vision and language capabilities. However, they lack the


capability to generate images. Some works propose integrat-


ing large language models (LLMs) with diffusion models


to equip LLMs with image generation capability (Sun et al.,


2024a; Ge et al., 2024b; Wu et al., 2024b). Others use


discrete tokens to support both image and text generation


simultaneously (Team, 2024a; Lu et al., 2024; Xie et al.,


2024b). They focus on multi-modal generation with lim-


ited image-generation capability. Concurrent works such as


TransFusion (Zhou et al., 2024a) and Show-O (Xie et al.,


2024b) unify diffusion and autoregressive methods into a


single model, generating text autoregressively and images


through diffusion. Nonetheless, like existing diffusion mod-


els, these works focus on a limited range of image generation


tasks, primarily text-to-image generation, and cannot cover


more complex and various visual generation tasks.


**Diffusion Model.** Recent advancements in diffusion models


have been remarkable, with notable contributions from the


Stable Diffusion (SD) series (Rombach et al., 2022; Podell


et al., 2024; Esser et al., 2024a), DALL-E (Ramesh et al.,


2022), and Imagen (Ho et al., 2022). These models are


predominantly designed for text-to-image generation tasks.


Many efforts have been made to extend the capabilities


of diffusion models, including ControlNet (Zhang et al.,


2023), T2I-Adapter (Mou et al., 2024), and StyleShot (Gao


et al., 2024). InstructPix2Pix (Brooks et al., 2023), and


EMU-edit (Sheynin et al., 2024) explore performing general


image editing tasks through instructions. However, these


methods are task-specific, extending the capabilities of


SD by modifying the model architecture.


Some work


explores the unification of computer vision tasks (Gan et al.,


2023; Geng et al., 2024). However, these efforts primarily


focus on traditional vision tasks instead of general image


generation tasks and often underperform compared to


models specifically designed and trained for corresponding


tasks.


**Chain of Thought (CoT).** CoT aims to improve the model’s


complex reasoning capabilities by correcting it step by step,


which has been widely explored in language models (Wei


et al., 2022a; Geva et al., 2021), yet still remains unexplored


in image generation. Recent MLLMs such as LLaVA-CoT


(Xu et al., 2024) leverage the LLMs to understand the visual


content. In this work, we introduce Multimodal Chain of


Thought to reveal the multimodal potential of a unified gen-


erative model, particularly for enhancing image generation.


**3. Method**


In this section, we will introduce the specific methodological


details of the MINT model. In Section 3.1, we present


the preliminaries, explaining the fundamental principles of


text generation and image generation, and introducing the


basic symbolic definitions. Section 3.2 elaborates on the


architectural design of the MINT model, explaining the


principles and functions of each module. Section 3.3 details


the steps of the MCoT for image generation. In Appendix A


and B, we present the detail of training strategy and the


methodology for constructing the data required for training.


**3.1. Preliminaries**


**Language Modeling.** Let *z* = (*z*1*, . . . , z**N*) *∈**V**N*denote


discrete token sequences with vocabulary *V* . Autoregressive


language models implement the joint distribution through


causal factorization:


*P*(*z*) =


*N*


Y


*i*=1


*P**θ*(*z**i* *|* *z**<i*)


(1)


where *θ* parameterizes the conditional distributions. Train-


ing minimizes the language modeling objective:


*L*LM = E*z**∼D*


"


*−*


*N*


X


*i*=1


log *P**θ*(*z**i* *|* *z**<i*)


#


(2)


For multimodal data pairs (*x, c*) with images *x* and captions


*c*, we extend the condition to *P**θ*(*z**i**|**z**<i**, ϕ*(*x*)), where *ϕ*(*·*)


encodes visual features.


**Rectified Flow.** Given image data **x** *∼D* and noise ***ϵ*** *∼*


*N*(0*, I*), we construct linear trajectories:


**x***t* = *t***x** + (1 *−**t*)***ϵ****,*


*t* *∈*[0*,* 1]


(3)


The velocity field model *v**θ* predicts straightening directions


through:


*L*RF = E*t**∼**U*(0*,*1)*,***x***,****ϵ****,c*





*∥*(**x** *−****ϵ***) *−**v**θ*(**x***t**, t, c*)*∥*2


2





(4)


which contrasts with the stochastic differential equations


used in Denoising Diffusion Probabilistic Models (DDPM)


by employing deterministic optimal transport paths.


4




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


















**Gen. Image 1**


**Artifact Map**


**Masked Image**


**Gen. Image 2**


**Reflection**


**Correction**





**"dense caption":**


*"The image depicts a modern, minimalist setting with a sleek, black rectangular* 


*table as the central focus. The overall atmosphere is clean, highlighted by a cute,* 


*fluffy purple dog sitting playfully on the table. The background features a wooden* 


*wall with vertical paneling. In front of the table, there are two chairs with a light-*


*colored fabric, suggesting a contemporary design aesthetic."*


**"bbox":** 


[{"a purple dog.": "0.293,0.418,0.684,0.74"}, {"a black dining table.": 


"0.125,0.525,0.996,0.99"}]


Generate image with caption: An 


AI Generated Image


*a photo of a purple dog above a* 


*black dining table*


Plan then generate: give dense 


caption and layout then draw


**"dense caption":** 


*"The image shows three small birds perched on a branch. The birds have* 


*distinctive plumage patterns, including orange and gray feathers, black beaks, and* 


*bright blue wings. They are sitting in a row, facing forward, and appear to be* 


*observing their surroundings. The background is blurred, focusing attention on the* 


*birds."*


**"bbox":** 


[{"three birds.": "0.648,0.359,0.824,0.711"}, {"three birds.":


"0.268,0.361,0.447,0.707"}, {"three birds.": "0.451,0.379,0.639,0.709"}]


Generate image with caption: An 


AI Generated Image


*a photo of three birds*


Plan then generate: give dense 


caption and layout then draw


**"dense caption":** 


“*The image shows a person sitting at a table with a shiny red apple in front of him.* 


*The background is softly blurred, suggesting a cozy indoor setting, possibly a cafe* 


*or restaurant, with warm lighting and a welcoming ambiance. The person is* 


*dressed casually, wearing a comfortable blue sweater and glasses, and his short* 


*hair frames his face. The scene captures a moment of casual enjoyment, as the* 


*apple adds a pop of color to the minimalist arrangement on the table.*”


**"bbox":** 


[{"an apple.": "0.564,0.703,0.824,0.975"}, {"a person.": "0.199,0.082,0.994,0.99"}]


Generate image with caption: An 


AI Generated Image


*a photo of a person and an apple*


Plan then generate: give dense 


caption and layout then draw


**Input**


**Planning**


**Acting**


Figure 4: Comprehensive examples for each step of Multi-modal Chain of Thought (MCoT).


**3.2. MINT Architecture Design**


3.2.1. INPUT REPRESENTATION


Text inputs ***T*** are transformed into an embedding sequence


***x***text *∈*R*L**×**d*using Qwen2’s tokenizer (Bai et al., 2023),


where ***L*** is the sequence length and ***d*** s the embedding


dimension.


An image ***I*** *∈*R*H**×**W* *×*3, with height ***H***


and width ***W*** , is encoded into a latent representation us-


ing SD3’s Variational Autoencoder (VAE) (Esser et al.,


2024b). Following the approach from Transfusion (Zhou


et al., 2024b), we compress 2*×*2 patches into a single vector,


resulting in image tokens ***x***image *∈*R


*H*


16*×* *W*


16*×**d*after linear


projection, where each token corresponds to a 16 *×* 16 pixel


patch of the original image. To accommodate and differen-


tiate the newly introduced image representations, we have


expanded the original vocabulary of Qwen2, which consists


of 151,936 entities, by incorporating 6 functional special


tokens.


3.2.2. MIXTURE OF TRANSFORMER EXPERTS


**Experts Design.** We design the mixture of transformer


experts (MTXpert) module as the core of the MINT model,


which is composed of parallel-arranged experts optimized


according to their respective training objectives, thereby


alleviating conflicts caused by the coupling of modalities


and capabilities (Liang et al., 2024a), and then facilitating


the expansion to additional new modalities.


Specifically, MTXpert comprises a Linguistic Expert, a Gen-


erative Vision Expert, and a Semantic Vision Expert, all of


which have *L* layers and fundamentally consistent structures.


Linguistic Expert is essentially the same as Qwen2, initial-


ized using the pre-trained Qwen2-0.5B model, to preserve


capabilities in language understanding, language generation,


multilingual proficiency, and reasoning. The initialization


method for the other two visual experts, as well as the de-


tailed model training strategy, are elaborated in Appendix A.


Each expert has independent weights for all non-embedding


model parameters, including projection matrices in the atten-


tion module, feed-forward networks, and layer normaliza-


tion, yet sharing the multimodal routing module and global


multimodal attention module.


Without loss of generality, we denote ***x*** = ***x****T* *⊕****x****C* *⊕****x****N* as


the input, respectively text tokens ***x****text*, clean image tokens


***x****clean*, noise ***x****noised* as ***x****T* *,* ***x****C**,* ***x****N*, and then describe


MTXpert in a configuration with a single layer follows:


ˆ


***x****T* *,* ˆ


***x****C**,* ˆ


***x****N* = ***Router***(***LN***(***x****T* *⊕****x****C* *⊕****x****N*))


ˆ


***x****q*


*T**,* ˆ


***x****k*


*T**,* ˆ


***x****v*


*T* = *W* *Q*


*T* ( ˆ


***x****T* )*, W**K*


*T* ( ˆ


***x****T* )*, W**V*


*T* ( ˆ


***x****T* )


ˆ


***x****q*


*C**,* ˆ


***x****k*


*C**,* ˆ


***x****v*


*C* = *W* *Q*


*C* ( ˆ


***x****C*)*, W**K*


*C* ( ˆ


***x****C*)*, W**V*


*C* ( ˆ


***x****C*)


ˆ


***x****q*


*N**,* ˆ


***x****k*


*N**,* ˆ


***x****v*


*N* = *W* *Q*


*N* ( ˆ


***x****N*)*, W**K*


*N* ( ˆ


***x****N*)*, W**V*


*N* ( ˆ


***x****N*)


ˆ


***x****rep*=


ˆ


***x****rep*


*T*


*⊕*


ˆ


***x****rep*


*C*


*⊕*


ˆ


***x****rep*


*N**, rep* *∈**q, k, v*


ˆ


***x*** = ***Attn***( ˆ


***x****q**,* ˆ


***x****k**,* ˆ


***x****v*) + ***x***


(5)


where *⊕*indicates concat operation, *blue*, *red*, *green*


respectively for Linguistic Expert, Semantic Visual Expert


and Generative Visual Expert modules. And ***Attn*** module


refers to Multimodal Attention which is explained in next


section. Then following FFN module can be expressed as:


ˆ


***x****T* *,* ˆ


***x****C**,* ˆ


***x****N* = ***Router***(***LN***(***x****T* *⊕****x****C* *⊕****x****N*))


ˆ


***x****T* *,* ˆ


***x****C**,* ˆ


***x****N* = *FFN* *T* ( ˆ


***x****T* )*, FFN* *C* ( ˆ


***x****C*)*, FFN* *N* ( ˆ


***x****N*)


ˆ


***x*** = ˆ


***x****T* *⊕*ˆ


***x****C* *⊕*ˆ


***x****N*


(6)


5




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


**Multimodal Attention.**


Despite the decoupling of ex-


pert parameters, we engage all experts in deep interaction


and alignment at each layer through a multimodal attention


mechanism. We explain the multimodal attention design


from both local and global perspectives. (a) At the local


level, we employ different attention patterns for the lan-


guage and vision modalities. For the language modality, the


attention mask adheres to the causal attention pattern. In


the case of the vision modality, which includes both clean


and noisy images, we use bidirectional attention among all


tokens of each individual image since images are physically


continuous representations and do not have a natural causal


sequence order. (b) At the global level, the global attention


mask for all modal data follows the causal attention pattern


based on their sequential arrangement, facilitating efficient


computation of loss and gradients over an entire sequence in


a single forward-backward pass without leaking information


from future tokens.


MTXpert, parallel-arranged experts structure, imposes no


limit on the potential of newly introduced modality capabil-


ities while seamlessly preserving understanding, generation,


and reasoning abilities of the original pre-trained language


model. The multimodal attention mechanism facilitates


deep modality interaction and alignment among the experts,


thereby supporting profound multimodal thinking and rea-


soning which are vital for MCoT. Consequently, MTXpert


demonstrates exceptional potential in addressing complex


multimodal tasks and expanding modal capabilities.


**3.3.** **Multimodal Chain of Thought for Enhanced Image**


**Generation**


For the first time, We introduce the MCoT training paradigm


to activate the native MCoT capabilities of unified genera-


tive models, facilitating step-by-step multimodal thinking,


reasoning, and reflection specifically designed to enhance


image generation. Similar to CoT, we define MCoT as a


series of multimodal thinking and reasoning intermediate


steps: Planning, Acting, Reflection, and Correction, aiming


at explicitly unlocking fine-grained multimodal alignment


and understanding, thereby improving thinking and reason-


ing abilities.


MCoT replaces the straightforward T2I generation process


with multiple reasoning steps. Specifically, the four steps


are grouped into Planning and Acting, as well as Reflection


and Correction, based on the two key image generation steps


in the foundational process. In Section 3.3.1, we elaborate


on the planning and acting processes that address most is-


sues related to intricate image generation with interwoven


conditions during the first stage. Meanwhile, the reflection


and correction processes focus on resolving region artifacts


and alignment discrepancies within the image, as discussed


in Section 3.3.2.


3.3.1. PLANNING AND ACTING


Generating intricate images—such as those featuring mul-


tiple objects, complex spatial relationships, and various at-


tributes that are all interwoven—requires nuanced, element-


wise decoupled alignment and a deep understanding of tex-


tual and visual components. However, straightforward T2I


generation are inadequate to effectively address these chal-


lenges.


To explicitly improve the fine-grained, element- wise decou-


pled alignment between concepts and attributes in images


with their corresponding textual elements, we design the


planning and acting steps. Specifically, during the plan-


ning step, as illustrated in Figure 4, MINT drafts a more


comprehensive and precise image caption without distorting


the meaning of the input prompt. In detail, MINT per-


forms both caption planning and layout planning. Caption


planning enhances image generation quality through more


detailed and comprehensive descriptions, while layout plan-


ning generates the spatial layout based on the relationships


between concepts, represented by bounding boxes for each


object’s position. These two steps foster textual CoT and


cross-modal CoT for conceptual and spatial visual aware-


ness, thereby enhancing the model’s inherent capability for


joint text-image understanding, mulitmodal reasoning par-


ticularly in detail drafting and spatial reasoning.


Subsequently, in the acting step, MINT generates images


informed by the dense draft developed during the planning


stage, effectively reducing the confusion often linked to


short captions and enhancing the quality of intricate im-


age generation, thereby promoting adherence to interwoven


conditions. Ultimately, these two steps further bolster the


model’s capacity to fully grasp the semantic connections


among the visual concepts and elements, along with their


corresponding text, enabling the generated images that are


firmly grounded in the logical relationships between them.


3.3.2. REFLECTION AND CORRECTION


Despite employing the aforementioned planning and acting


operations to enhance image generation capabilities, the


model still struggles to render all parts of an image per-


fectly in a single generation attempt, following issues such


as artifacts, low aesthetic quality, and discrepancies with the


corresponding caption. To address these challenges, we pro-


pose the reflection step, which enable MINT to self-assess


regions of low aesthetic quality, self-reflect artifacts within


the generated image, and identify discrepancies between


the image and the corresponding caption. Build upon this,


the correction step optimizes the image using the insights


gained from the reflection step.


Specifically, as illustrated in Figure 3 and 4, during the


reflection step, MINT leverages the generated image and


6




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


**Layout Planning and Following**




"bbox": 


[{"a vase.": "0.617,0.482,0.832,0.92"}, 


{"a broccoli.": "0.025,0.416,0.486,0.91"}]


"bbox": 


[{"a vase.": "0.129,0.576,0.357,0.949"}, 


{"a broccoli.": "0.408,0.568,0.787,0.951"}]


"bbox": 


[{"a vase.": "0.512,0.453,0.809,0.879"}, 


{"a broccoli.": "0.229,0.291,0.619,0.844"}]


a broccoli and a vase


The image shows a bowl of food on a 


wooden table. The bowl contains a variety 


of ingredients, including diced tomatoes, 


sliced onions, and some green leafy herbs. 


The bowl is placed on a white cloth. In 


the background, there is a small bowl 


with what appears to be a yellowish 


substance, possibly a condiment or a side 


dish. The overall setting suggests a cozy 


and rustic dining atmosphere.





a bowl






a train


**Detail Planning and Following**


**Artifact Reflection and Correction**


Figure 5: New capabilities integrated into MINT through


MCoT. Layout planning enhances spatial reasoning and then


adherence to fine-grained positioning instructions, while


detail planning improves detail drafting by automatically


adding rich details to images, preserving their original mean-


ing and thereby enhancing image quality. Furthermore, the


model features self-reflection capabilities that enable it to


evaluate and correct image artifacts in a human-like manner,


further optimizing image quality.


the input prompt as inputs to identify regions exhibiting


artifacts, low aesthetic quality, and misalignments with the


input prompt. This process produces an artifact heatmap


that quantifies the confidence scores of areas requiring cor-


rection, wherein a higher confidence score signifies a more


pronounced necessity for adjustments. Furthermore, the


reflection process empowers MINT to unlock cross-modal


chain-of-thought (CoT) reasoning for both image evaluation


and text-image element-wise alignment, and to self-reflect in


a human-like manner for leading to more accurate, reliable,


and aesthetically pleasing results.


In the correction step, MINT integrates artifact reflection


and planning rationales into the generation process, as illus-


trated in Figure 3, and performs targeted inpainting correc-


tions to refine the artifacts regions that are masked in image


based on the artifact heatmap. This process endows MINT


with inpainting capability, ultimately effectively addressing


issues such as artifacts and aesthetic deficiencies and then


producing a higher-quality image that aligns more closely


with the intended vision and corresponding caption.


**4. Experiments**


**4.1. Experimental Details**


**Implemental Details.** We employ AdamW (Loshchilov


& Hutter, 2017) as the optimizer with parameter setting of


*β*1 = 0*.*9*, β*2 = 0*.*999, and a weight decay of 0.02. The


learning rate is configured to a constant value of 5 *×* 10*−*5,


incorporating a linear warm-up phase over 10,000 steps.


The training utilized DeepSpeed’s ZeRO-2 (Rajbhandari


et al., 2020) optimization.


**Evaluation Benchmarks.** We rigorously evaluate the per-


formance of our model using a comprehensive suite of


benchmarks tailored for assessing both image generation


and image understanding capabilities. (I) Image Genera-


tion Evaluation: We employ the MS-COCO dataset (Lin


et al., 2014) alongside two key metrics: Fr´


echet Inception


Distance (FID) (Heusel et al., 2017) and CLIP Score (Rad-


ford et al., 2021). Additionally, we leverage the GenEval


benchmark (Ghosh et al., 2024) to further assessment of


enhanced image generation capability, as it offers a holistic


framework for assessing generative models across various


dimensions. (II) Image Understanding Evaluation: We ref-


erence a series of benchmarks that include the MS-COCO


with CIDEr score (Vedantam et al., 2015), VQA-v2 (Goyal


et al., 2017), MME (Fu et al., 2024), and MMBench (Liu


et al., 2024d).


**4.2. Evaluation for Enhanced Image Generation**


The results presented in Table 1 provide a comprehensive


evaluation of enhanced image generation capabilities across


various models, including image-only unimodal generative


models and unified generative models. The upper half of the


table highlights unimodal models (image-only), while the


lower half features multimodal models capable of generating


both images and text.


MINT, our proposed method, achieves an overall score of


0.73, surpassing all other models in its category. For the


single-object generation task, MINT scores 0.98, match-


ing the highest scores from unimodal models like SD v2.1


(0.98) and SD 3 (0.98). This performance reflects its ability


to accurately generate detailed representations of isolated


subjects, likely due to the dense caption planning utilized in


the MCoT framework.


In the two-object generation task, MINT scores 0.82, signif-


icantly outperforming models such as DALL-E 2 (0.66) and


SEED-X (0.58). This demonstrates MINT’s robust capabil-


ity in composing complex scenes that involve interactions


between multiple subjects. Regarding the counting task,


MINT achieves a score of 0.66, which is notably superior


to other unified generative models and comparable to SD 3,


indicating its enhanced ability to accurately manage and


represent quantities of generated objects.


7




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


Table 1: **Comparison of enhanced image generation quality on GenEval benchmark.** ”Uni.” refers to unimodal


generative models that operate exclusively on images, while ”Multi.” indicates multimodal generative models that are


capable of generating both images and text.


**Model**


**Params**


**Type**


**Overall***↑*


**Single Obj.**


**Two Obj.**


**Counting**


**Colors**


**Position**


**Attr. Binding**


SD v1.5 (Rombach et al., 2022)


1B


Uni.


0.43


0.97


0.38


0.35


0.76


0.04


0.06


SD v2.1 (Rombach et al., 2022)


1.3B


Uni.


0.50


0.98


0.51


0.44


0.85


0.07


0.17


SD-XL (Podell et al., 2024)


3.4B


Uni.


0.55


0.98


0.74


0.39


0.85


0.15


0.23


SD 3 (Esser et al., 2024a)


12.7B


Uni.


0.68


0.98


0.84


0.66


0.74


0.40


0.43


DALL-E 2 (Ramesh et al., 2022)


4.5B


Uni.


0.52


0.94


0.66


0.49


0.77


0.10


0.19


DALL-E 3 (Betker et al., 2023)


–


Uni.


0.67


0.96


0.87


0.47


0.83


0.43


0.45


IF-XL (DeepFloyd, 2023)


10.1B


Uni.


0.61


0.97


0.74


0.66


0.81


0.13


0.35


Chameleon (Team, 2024a)


34B


Multi.


0.39


–


–


–


–


–


–


Transfusion (Zhou et al., 2024a)


7.3B


Multi.


0.63


–


–


–


–


–


–


LWM (Liu et al., 2024c)


7B


Multi.


0.47


0.93


0.41


0.46


0.79


0.09


0.15


SEED-X (Ge et al., 2024a)


17B


Multi.


0.49


0.97


0.58


0.26


0.80


0.19


0.14


Show-o (Xie et al., 2024a)


1.3B


Multi.


0.53


0.95


0.52


0.49


0.82


0.11


0.28


Janus (Wu et al., 2024a)


1.3B


Multi.


0.61


0.97


0.68


0.30


0.84


0.46


0.42


JanusFlow (Ma et al., 2024)


1.3B


Multi.


0.63


0.97


0.59


0.45


0.83


0.53


0.42


**MINT (Ours)**


1.3B


Multi.


**0.73**


**0.98**


**0.82**


**0.66**


0.79


**0.55**


**0.56**


Table 2: **Performance of foundational image generation**


**capabilities on the MS COCO benchmark.** The asterisk


(\*) indicates that the CIDEr results are calculated based on


30K randomly sampled data from the MS-COCO validation


set, rather than the standard Karpathy test split set.


**Model**


**Params**


**FID***↓*


**CLIP Score** *↑*


**CIDEr***↑*


*Uni-modal Generative Model*


SD v1.5 (Rombach et al., 2022)


1B


9.93


30.2


–


SD-XL (Podell et al., 2024)


3.4B


–


31.0


–


DALL-E 2 (Ramesh et al., 2022)


4.5B


10.39


31.4


–


DALL-E 3 (Betker et al., 2023)


–


–


32.0


–


IF-XL (DeepFloyd, 2023)


10.1B


6.66


–


–


Emu2-GEN (Sun et al., 2024a)


37B


–


29.7


–


*Multi-modal Generative Model*


DREAMLLM (Dong et al., 2024)


7B


–


–


115.4


Chameleon (Team, 2024b)


7B


26.74


24.3


120.2


Transfusion (Zhou et al., 2024b)


7.3B


6.78


25.5


32.0\*


SEED-LLaMA (Ge et al., 2023)


8B


–


–


123.6


MetaMorph (Tong et al., 2024)


8B


11.8


–


–


Emu 3 (Wang et al., 2024b)


8B


12.8


31.3


-


LLamaFusion (Shi et al., 2024)


8B


8.61


24.4


38.4\*


LLaVAFusion (Shi et al., 2024)


8B


8.28


24.7


–


NExT-GPT (Wu et al., 2024b)


13B


10.07


–


124.9


Show-o (Xie et al., 2024a)


1.3B


9.24


–


–


Janus (Wu et al., 2024a)


1.3B


8.5


–


–


**MINT (Ours)**


1.3B


7.83


25.2


125.3


In positioning tasks, MINT excels over DALL-E-3 (0.43),


showcasing a solid understanding of spatial relationships


among objects. Its impressive performance in generating im-


ages with multiple objects and diverse spatial arrangements


can be attributed to the inherent layout planning capabilities


derived from the native MCoT framework. By leveraging


its robust multimodal understanding and reasoning abilities,


MINT effectively plans and arranges object placements in


advance, ensuring consistency in both quantity and position-


ing.


In terms of color generation, MINT scores 0.79, which is


Table 3: **Comparison results of foundational image un-**


**derstanding performance.**


**Model**


**Params**


**MME-P***↑*


**MMBench***↑*


**VQA-v2***↑*


*Uni-modal Generative Model*


MobileVLM (Chu et al., 2023)


2.7B


1288.9


59.6


–


LLaVA-Phi (Zhu et al., 2024)


2.7B


1335.1


59.8


71.4


LLaVA (Liu et al., 2024b)


7B


809.6


38.7


–


LLaVA-v1.5 (Liu et al., 2024a)


7B


1510.7


64.3


78.5


Qwen-VL-Chat (Bai et al., 2023)


7B


1487.5


60.6


78.2


IDEFICS-9B (Laurenc


¸on et al., 2023)


8B


–


48.2


50.9


Emu3-Chat (Wang et al., 2024b)


8B


–


58.5


75.1


InstructBLIP (Dai et al., 2023)


13B


1212.8


–


–


LLaVA-v1.5-Phi-1.5 (Xie et al., 2024a)


1.3B


1128.0


–


75.3


MobileVLM (Chu et al., 2023)


1.4B


1196.2


53.2


–


MobileVLM-V2 (Chu et al., 2024)


1.4B


1302.8


57.7


–


*Multi-modal Generative Model*


LWM (Liu et al., 2024c)


7B


–


–


55.8


VILA-U (Wu et al., 2024c)


7B


1401.8


–


79.4


LaVIT (Jin et al., 2024)


7B


–


–


66.0


Chameleon(Team, 2024a)


7B


–


35.7


–


Emu (Sun et al., 2024b)


13B


–


–


52.0


NExT-GPT (Wu et al., 2024b)


13B


–


–


66.7


LLaVAFusion (Shi et al., 2024)


–


–


72.1


–


Gemini-Nano-1 (Team, 2023)


1.8B


–


–


62.7


Show-o (Xie et al., 2024a)


1.3B


948.4


–


59.3


JanusFlow (Ma et al., 2024)


1.3B


1333.1


74.9


79.8


Janus (Wu et al., 2024a)


1.3B


1338.0


69.4


77.3


**MINT (Ours)**


1.3B


1335.2


70.6


78.3


comparable to the highest-scoring model, SD-XL (0.85), de-


spite having fewer parameters (1.3B vs. 3.4B). This reflects


MINT’s ability to produce rich and diverse color schemes


that enhance the visual quality of generated images.


Finally, in attribute binding tasks, MINT scores 0.56, repre-


senting a 24.4% improvement over DALL-E 3 (0.45) and


significantly exceeding all other unimodal and unified gen-


erative models. This highlights MINT’s enhanced capability


to manage the complexities of attribute composition, demon-


strating superior consistency in text-image alignment, which


is crucial for unified generative applications.


In summary, MINT, with its inherent MCoT capabilities,


demonstrates outstanding performance across various image


8




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


generation tasks, particularly in enhancing the generation of


complex images that require nuanced alignment and reason-


ing between text and visuals.


**4.3.** **Foundational Image Generation and Understanding**


**Image Generation.** We utilize the MS-COCO dataset to


evaluate the foundational image generation capabilities of


various generative models. As shown in Table 2, MINT,


with a parameter count of 1.3 billion, stands out among


its competitors. Notably, MINT achieves a FID score of


7.83, which is significantly lower than many unimodal and


multimodal generative models, such as DALL-E 2 (FID of


10.39) and NExT-GPT (FID of 10.07). This suggests that


MINT excels in generating higher-quality images. In terms


of the CLIP score, MINT scores 25.2, which is comparable


to Transfusion’s score of 25.5, despite having considerably


fewer parameters (1.3B vs. 7.3B). This indicates that MINT


maintains a strong correlation with text.


Moreover, MINT achieved a CIDEr score of 125.3, sur-


passing all other models in comparison, particularly demon-


strating significant advantages in multimodal generative


frameworks. This outcome highlights MINT’s capability to


generate images that are highly consistent with text descrip-


tions, reinforcing its effectiveness in generative tasks.


**Image Understanding.** As shown in Table 3, we present


the results of our experiments on various image understand-


ing benchmarks, comparing MINT against several state-


of-the-art models to demonstrate its efficacy in the field


of image understanding tasks. MINT achieves an impres-


sive MME-P score of 1335.2, surpassing several unimodal


generative models with comparable parameter counts, such


as MobileVLM-V2 and Show-o, and closely aligning with


leading models despite having only 1.3 billion parameters.


This underscores MINT’s effectiveness in tackling complex


image understanding challenges. In the MMBench evalua-


tion, MINT scores 70.6, outperforming notable competitors


like Janus and LLaVA-v1.5, which highlights its strong


capabilities in multimodal scenarios.


Furthermore, MINT attains a VQA-v2 score of 78.3, po-


sitioning it among the top performers in visual question


answering and directly competing with robust models like


LLaVA-v1.5 and Qwen-VL-Chat. Overall, MINT demon-


strates exceptional performance across various image under-


standing benchmarks, establishing itself as a highly competi-


tive model within both unimodal and multimodal generative


frameworks.


**4.4. Ablation Study**


In our ablation study, we evaluate the effectiveness of MCoT


for enhanced image generation across various configura-


tions, as detailed in Table 4. Since MCoT training is built


Table 4: **Ablation results on GenEval Benchmark for**


**validating the effectiveness of MCoT.**


**Task**


**Generation Style**


T2I


*Gen. Twice*


MCoT


*Planning & Acting Only*


MCoT


*Full Process*


Single Obj.


0.97


0.97


0.98


Two Obj.


0.80


0.84


0.82


Counting


0.58


0.63


0.66


Colors


0.78


0.81


0.79


Position


0.40


0.50


0.55


Attr. Binding


0.47


0.53


0.56


Overall


0.67


0.71


0.73


upon pretrained MINT model, to ensure a fair comparison,


we conduct additional training steps for the baseline using


the basic multimodal training paradigm (T2I and I2T) on


the same checkpoint, effectively eliminating potential biases


arising from increased training iterations. Moreover, as the


full MCoT process involves two image generation steps, we


maintain fairness by requiring the baseline model to per-


form T2I generation twice during testing and selecting the


best result as the foundational reference for subsequent ex-


periments. The baseline model ultimately yields an overall


score of 0.67 on GenEval benchmark.


**Planning and Acting.** Building upon the same pretrained


MINT model, we conduct MCoT training and utilize only


planning and acting steps for image generation. Notably, in


this configuration, image generation is performed only once.


As shown in the second column, we observe a significant


performance improvement, with the score increasing from


0.67 to 0.71. This underscores the effectiveness of incor-


porating planning and acting mechanisms in enhancing the


model’s capability to generate intricate images. We attribute


this improvement to three key factors: (1) a comprehensive


understanding of textual and visual components, (2) explicit


and nuanced element-wise decoupled alignment, and (3)


advanced multimodal reasoning capabilities, particularly in


spatial reasoning.


**Reflection and Correction.** Furthermore, with the inclu-


sion of the *Reflection & Correction* mechanism, as shown


in the last column, the final performance increases to 0.73.


This demonstrates that the reflection and correction steps


significantly enhance the quality of the generated results.


We attribute this improvement to the model’s self-reflection


capability, which enables it to identify artifacts within the


image, assess discrepancies between the image and the


text prompt, and automatically implement corrective ad-


justments.


**4.5. Qualitative Analysis**


**MCoT v.s. Straightforward T2I.** As illustrated by the


failed results in the first row of Figure 2, several issues


9




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


arise: (1) concept confusion in multiple-object generation,


(2) spatial confusion in layout instruction generation, (3)


attribute confusion in interwoven condition generation, and


(4) partial object defects.


Specifically, we attribute concept confusion to the scarcity


of uncommon object combinations in nature. Addressing


this issue requires the model to possess a comprehensive and


precise understanding of object concepts in both the textual


and visual domains, which is enhanced by our MCoT frame-


work through explicit fine-grained alignment. Additionally,


spatial confusion stems from weak spatial perception and


misalignment between fine-grained layout instructions and


their corresponding visual outputs. This is mitigated by the


layout planning and acting steps in MCoT, which endow


MINT with spatial reasoning capabilities.


Moreover, in mixed-attribute image generation tasks, the


interleaving of multiple attributes can hinder the model’s


ability to accurately follow each detail. MCoT addresses


this challenge through nuanced element-wise decoupled


alignment and advanced multimodal reasoning. Finally,


due to the complexity and abundance of elements in gener-


ated images, single-pass generation can result in regional


defects. MINT, equipped with reflection and correction


mechanisms, demonstrates a clear advantage in this regard,


as it significantly improves image quality through automatic


assessment and correction at the regional detail level.


**5. Conclusions**


We present ***MINT***, an advanced unified generative model


that leverages a native multimodal chain of thought (MCoT)


to enhance image generation. Our model incorporates a


Mixture of Transformer Experts (MTXpert) architecture


that effectively balances natural language processing and


visual capabilities. By implementing an innovative MCoT


training paradigm, MINT achieves nuanced element-wise


alignment and promotes advanced multimodal reasoning


and self-reflection. As a result, MINT demonstrates superior


performance in text-to-image (T2I) and image-to-text (I2T)


tasks across multiple benchmarks, addressing the challenges


of generating intricate images.


**References**


Bachmann, R., Kar, O. F., Mizrahi, D., Garjani, A., Gao,


M., Griffiths, D., Hu, J., Dehghan, A., and Zamir, A.


4m-21: An any-to-any vision model for tens of tasks and


modalities. *arXiv preprint arXiv:2406.09406*, 2024.


Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J.,


Zhou, C., and Zhou, J. Qwen-VL: A frontier large vision-


language model with versatile abilities. *arXiv preprint*


*arXiv:2308.12966*, 2023.


Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,


Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving


image generation with better captions. *Computer Science*,


2023.


Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix:


Learning to follow image editing instructions. In *CVPR*,


pp. 18392–18402, 2023.


Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S.,


Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl:


Scaling up vision foundation models and aligning for


generic visual-linguistic tasks.


In *CVPR*, pp. 24185–


24198, 2024.


Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei,


F., Zhang, X., Zhang, B., Wei, X., et al. MobileVLM: A


fast, reproducible and strong vision language assistant for


mobile devices. *arXiv preprint arXiv:2312.16886*, 2023.


Chu, X., Qiao, L., Zhang, X., Xu, S., Wei, F., Yang, Y., Sun,


X., Hu, Y., Lin, X., Zhang, B., et al. MobileVLM V2:


Faster and stronger baseline for vision language model.


*arXiv preprint arXiv:2402.03766*, 2024.


Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,


W., Li, B., Fung, P., and Hoi, S. InstructBLIP: Towards


general-purpose vision-language models with instruction


tuning. In *Proc. Annu. Conf. Neural Inf. Process. Systems*,


2023.


DeepFloyd.


DeepFloyd IF, 2023.


URL https:


//huggingface.co/DeepFloyd/IF-I-XL-v1.


0.


Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin,


D., Lin, J., Zou, X., Shao, Z., Yang, H., et al. Cogview:


Mastering text-to-image generation via transformers. In


*NeurIPS*, 2021.


Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J.,


Zhao, L., Sun, J., Zhou, H., Wei, H., et al. DreamLLM:


Synergistic multimodal comprehension and creation. In


*Proc. Int’l Conf. Learning Representations*, 2024.


Esser, P., Kulal, S., Blattmann, A., Entezari, R., M¨


uller, J.,


Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al.


Scaling rectified flow transformers for high-resolution


image synthesis. In *Proc. Int’l Conf. Machine Learning*,


2024a.


Esser, P., Kulal, S., Blattmann, A., Entezari, R., M¨


uller, J.,


Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al.


Scaling rectified flow transformers for high-resolution


image synthesis. In *Forty-first International Conference*


*on Machine Learning*, 2024b.


10




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang,


J., Zheng, X., Li, K., Sun, X., Wu, Y., and Ji, R. MME:


A comprehensive evaluation benchmark for multimodal


large language models. *arXiv preprint arXiv:2306.13394*,


2024.


Gan, Y., Park, S., Schubert, A., Philippakis, A., and Alaa,


A. M. Instructcv: Instruction-tuned text-to-image dif-


fusion models as vision generalists.


*arXiv preprint*


*arXiv:2310.00390*, 2023.


Gao, J., Liu, Y., Sun, Y., Tang, Y., Zeng, Y., Chen, K., and


Zhao, C. Styleshot: A snapshot on any style. *arXiv*


*preprint arXiv:2407.01414*, 2024.


Ge, Y., Zhao, S., Zeng, Z., Ge, Y., Li, C., Wang, X., and


Shan, Y. Making llama see and draw with seed tokenizer.


*arXiv preprint arXiv:2310.01218*, 2023.


Ge, Y., Zhao, S., Zhu, J., Ge, Y., Yi, K., Song, L., Li, C.,


Ding, X., and Shan, Y. SEED-X: Multimodal models with


unified multi-granularity comprehension and generation.


*arXiv preprint arXiv:2404.14396*, 2024a.


Ge, Y., Zhao, S., Zhu, J., Ge, Y., Yi, K., Song, L., Li, C.,


Ding, X., and Shan, Y. Seed-x: Multimodal models with


unified multi-granularity comprehension and generation.


*arXiv preprint arXiv:2404.14396*, 2024b.


Geng, Z., Yang, B., Hang, T., Li, C., Gu, S., Zhang, T., Bao,


J., Zhang, Z., Li, H., Hu, H., et al. Instructdiffusion: A


generalist modeling interface for vision tasks. In *CVPR*,


pp. 12709–12720, 2024.


Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and


Berant, J. Did aristotle use a laptop? a question answering


benchmark with implicit reasoning strategies. *Transac-*


*tions of the Association for Computational Linguistics*, 9:


346–361, 2021.


Ghosh, D., Hajishirzi, H., and Schmidt, L. GenEval: An


object-focused framework for evaluating text-to-image


alignment. In *Proc. Annu. Conf. Neural Inf. Process.*


*Systems*, 2024.


Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and


Parikh, D. Making the v in VQA matter: Elevating the


role of image understanding in visual question answering.


In *Proc. IEEE Int’l Conf. Computer Vision and Pattern*


*Recognition*, 2017.


He, W., Fu, S., Liu, M., Wang, X., Xiao, W., Shu, F., Wang,


Y., Zhang, L., Yu, Z., Li, H., et al. Mars: Mixture of


auto-regressive models for fine-grained text-to-image syn-


thesis. *arXiv preprint arXiv:2407.07614*, 2024.


Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and


Hochreiter, S. GANs trained by a two time-scale update


rule converge to a local nash equilibrium. In *Proc. Annu.*


*Conf. Neural Inf. Process. Systems*, 2017.


Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,


A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,


et al. Imagen video: High definition video generation


with diffusion models. *arXiv preprint arXiv:2210.02303*,


2022.


Jin, Y., Xu, K., Chen, L., Liao, C., Tan, J., Huang, Q.,


Bin, C., Song, C., ZHANG, D., Ou, W., et al. Unified


language-vision pretraining in llm with dynamic discrete


visual tokenization. In *Proc. Int’l Conf. Learning Repre-*


*sentations*, 2024.


Ju, X., Liu, X., Wang, X., Bian, Y., Shan, Y., and Xu,


Q. Brushnet: A plug-and-play image inpainting model


with decomposed dual-branch diffusion. In *European*


*Conference on Computer Vision*, pp. 150–168. Springer,


2024.


Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J.,


Schindler, G., Hornung, R., Birodkar, V., Yan, J., Chiu,


M.-C., et al. Videopoet: A large language model for zero-


shot video generation. *arXiv preprint arXiv:2312.14125*,


2023.


Laurenc


¸on, H., van Strien, D., Bekman, S., Tronchon, L.,


Saulnier, L., Wang, T., Karamcheti, S., Singh, A., Pis-


tilli, G., Jernite, Y., et al. Introducing IDEFICS: An


open reproduction of state-of-the-art visual language


model, 2023, 2023. URL https://huggingface.


co/blog/idefics.


Liang, W., Yu, L., Luo, L., Iyer, S., Dong, N., Zhou, C.,


Ghosh, G., Lewis, M., Yih, W.-t., Zettlemoyer, L., et al.


Mixture-of-transformers: A sparse and scalable architec-


ture for multi-modal foundation models. *arXiv preprint*


*arXiv:2411.04996*, 2024a.


Liang, Y., He, J., Li, G., Li, P., Klimovskiy, A., Carolan,


N., Sun, J., Pont-Tuset, J., Young, S., Yang, F., et al.


Rich human feedback for text-to-image generation. In


*Proceedings of the IEEE/CVF Conference on Computer*


*Vision and Pattern Recognition*, pp. 19401–19411, 2024b.


Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,


Ramanan, D., Doll´


ar, P., and Zitnick, C. L. Microsoft


coco: Common objects in context. In *Proc. European*


*Conf. Computer Vision*, 2014.


Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction


tuning. In *NeurIPS*, 2023.


Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with


visual instruction tuning. In *Proc. IEEE Int’l Conf. Com-*


*puter Vision and Pattern Recognition*, 2024a.


11




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction


tuning. In *Proc. Annu. Conf. Neural Inf. Process. Systems*,


2024b.


Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model


on million-length video and language with ringattention.


*arXiv preprint arXiv:2402.08268*, 2024c.


Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,


Yuan, Y., Wang, J., He, C., Liu, Z., et al. MMBench:


Is your multi-modal model an all-around player?


In


*Proc. European Conf. Computer Vision*, 2024d.


Loshchilov, I. and Hutter, F. Decoupled weight decay regu-


larization. *arXiv preprint arXiv:1711.05101*, 2017.


Lu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten,


R., Hoiem, D., and Kembhavi, A. Unified-io 2: Scaling


autoregressive multimodal models with vision language


audio and action. In *CVPR*, pp. 26439–26455, 2024.


Ma, Y., Liu, X., Chen, X., Liu, W., Wu, C., Wu, Z., Pan,


Z., Xie, Z., Zhang, H., Zhao, L., et al. Janusflow: Har-


monizing autoregression and rectified flow for unified


multimodal understanding and generation. *arXiv preprint*


*arXiv:2411.07975*, 2024.


Mou, C., Wang, X., Xie, L., Wu, Y., Zhang, J., Qi, Z., and


Shan, Y. T2i-adapter: Learning adapters to dig out more


controllable ability for text-to-image diffusion models. In


*AAAI*, volume 38, pp. 4296–4304, 2024.


OpenAI. GPT-4 technical report. *arXiv:2303.08774*, 2023.


Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,


T., M¨


uller, J., Penna, J., and Rombach, R. SDXL: Im-


proving latent diffusion models for high-resolution image


synthesis. In *Proc. Int’l Conf. Learning Representations*,


2024.


Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,


Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,


J., et al. Learning transferable visual models from natu-


ral language supervision. In *Proc. Int’l Conf. Machine*


*Learning*, 2021.


Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero:


Memory optimizations toward training trillion parameter


models, 2020.


Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,


M. Hierarchical text-conditional image generation with


CLIP latents. *arXiv preprint arXiv:2204.06125*, 2022.


Ren, T., Liu, S., Zeng, A., Lin, J., Li, K., Cao, H., Chen,


J., Huang, X., Chen, Y., Yan, F., et al. Grounded sam:


Assembling open-world models for diverse visual tasks.


*arXiv preprint arXiv:2401.14159*, 2024.


Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and


Ommer, B. High-resolution image synthesis with latent


diffusion models. In *Proc. IEEE Int’l Conf. Computer*


*Vision and Pattern Recognition*, 2022.


Sheynin, S., Polyak, A., Singer, U., Kirstain, Y., Zohar, A.,


Ashual, O., Parikh, D., and Taigman, Y. Emu edit: Precise


image editing via recognition and generation tasks. In


*CVPR*, pp. 8871–8879, 2024.


Shi, W., Han, X., Zhou, C., Liang, W., Lin, X. V., Zettle-


moyer, L., and Yu, L. Llamafusion: Adapting pretrained


language models for multimodal generation.


*arXiv*


*preprint arXiv:2412.15188*, 2024.


Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Wang, Y.,


Rao, Y., Liu, J., Huang, T., and Wang, X. Generative


multimodal models are in-context learners. In *CVPR*, pp.


14398–14409, 2024a.


Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y.,


Gao, H., Liu, J., Huang, T., and Wang, X. Generative pre-


training in multimodality. In *Proc. Int’l Conf. Learning*


*Representations*, 2024b.


Team, C. Chameleon: Mixed-modal early-fusion foundation


models. *arXiv preprint arXiv:2405.09818*, 2024a.


Team, C. Chameleon: Mixed-modal early-fusion foundation


models. *arXiv preprint arXiv:2405.09818*, 2024b.


Team, G. Gemini: a family of highly capable multimodal


models. *arXiv preprint arXiv:2312.11805*, 2023.


Tong, S., Fan, D., Zhu, J., Xiong, Y., Chen, X., Sinha, K.,


Rabbat, M., LeCun, Y., Xie, S., and Liu, Z. Metamorph:


Multimodal understanding and generation via instruction


tuning. *arXiv preprint arXiv:2412.14164*, 2024.


Vedantam, R., Lawrence Zitnick, C., and Parikh, D. Cider:


Consensus-based image description evaluation. In *Pro-*


*ceedings of the IEEE conference on computer vision and*


*pattern recognition*, pp. 4566–4575, 2015.


Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen,


K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing


vision-language model’s perception of the world at any


resolution. *arXiv preprint arXiv:2409.12191*, 2024a.


Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji,


J., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual


expert for pretrained language models. *arXiv preprint*


*arXiv:2311.03079*, 2023.


Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang,


J., Zhang, F., Wang, Y., Li, Z., Yu, Q., et al. Emu3:


Next-token prediction is all you need. *arXiv preprint*


*arXiv:2409.18869*, 2024b.


12




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,


E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting


elicits reasoning in large language models. *Advances in*


*neural information processing systems*, 35:24824–24837,


2022a.


Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,


E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting


elicits reasoning in large language models. *Advances in*


*neural information processing systems*, 35:24824–24837,


2022b.


Wu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu,


W., Xie, Z., Yu, X., Ruan, C., et al. Janus: Decoupling


visual encoding for unified multimodal understanding and


generation. *arXiv preprint arXiv:2410.13848*, 2024a.


Wu, S., Fei, H., Qu, L., Ji, W., and Chua, T.-S. NExT-GPT:


Any-to-any multimodal LLM. In *Proc. Int’l Conf. Ma-*


*chine Learning*, 2024b.


Wu, Y., Zhang, Z., Chen, J., Tang, H., Li, D., Fang, Y., Zhu,


L., Xie, E., Yin, H., Yi, L., et al. VILA-U: A unified


foundation model integrating visual understanding and


generation. *arXiv preprint arXiv:2409.04429*, 2024c.


Xiao, S., Wang, Y., Zhou, J., Yuan, H., Xing, X., Yan, R.,


Wang, S., Huang, T., and Liu, Z. Omnigen: Unified image


generation. *arXiv preprint arXiv:2409.11340*, 2024.


Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q.,


Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One


single transformer to unify multimodal understanding and


generation. *arXiv preprint arXiv:2408.12528*, 2024a.


Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q.,


Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One


single transformer to unify multimodal understanding and


generation. *arXiv preprint arXiv:2408.12528*, 2024b.


Xu, G., Jin, P., Hao, L., Song, Y., Sun, L., and Yuan, L.


Llava-o1: Let vision language models reason step-by-


step. *arXiv preprint arXiv:2411.10440*, 2024.


Zhang, L., Rao, A., and Agrawala, M. Adding conditional


control to text-to-image diffusion models. In *ICCV*, pp.


3836–3847, 2023.


Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M.,


Shamis, L., Kahn, J., Ma, X., Zettlemoyer, L., and


Levy, O. Transfusion: Predict the next token and dif-


fuse images with one multi-modal model. *arXiv preprint*


*arXiv:2408.11039*, 2024a.


Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M.,


Shamis, L., Kahn, J., Ma, X., Zettlemoyer, L., and


Levy, O. Transfusion: Predict the next token and dif-


fuse images with one multi-modal model. *arXiv preprint*


*arXiv:2408.11039*, 2024b.


Zhu, Y., Zhu, M., Liu, N., Ou, Z., Mou, X., and Tang, J.


LLaVA-Phi: Efficient multi-modal assistant with small


language model. *arXiv preprint arXiv:2401.02330*, 2024.


13




**Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation**


**A. Train Strategy**


**A.1. Pre-training for T2I and I2T**


We adopt a two-stage training approach to systematically integrate the text-to-image (T2I) and image-to-text (I2T) capabilities


of the MINT model.


**Stage I**: In this initial stage, we focus on optimizing the T2I functionality of MINT by training the model specifically for the


T2I task. Both the Linguistic Expert and the Generative Visual Expert are trained, with their parameters initialized from the


Qwne2-0.5B model. The image resolution for this phase is configured to 256 *×* 256 pixels.


**Stage II**: During this stage, MINT undergoes mixed training, encompassing both T2I and I2T tasks at a training task ratio


of 8:1. The I2T tasks include image captioning and Visual Question Answering (VQA). In this phase, all experts are


trainable, and the newly introduced Semantic Visual Expert is initialized using parameters from the Generative Visual Expert


established in Stage I. The resolution of images at this stage is increased to 512 *×* 512.


**A.2. Multimodal Chain of Thought Training**


After the pretraining phase, MINT possesses capabilities for image generation and image understanding, enabled by the


any2any task paradigm supported within MNIT. We decompose the MCoT training into a multi-task format that separately


supports planning and acting, reflection, and correction. Specifically, for the planning and acting task, the model takes


a short caption as input and produces a dense caption along with the coordinates of the objects planned in the image.


For the reflection task, the model receives a generated image and short caption as input and predicts the corresponding


artifact/implausibility heatmap(Liang et al., 2024b). For the correction task, we achieve this through training on inpainting


tasks. It is noteworthy that all of these tasks are trained simultaneously within the same unified model.


**B. Dataset Construction**


**B.1. Pre-training Data**


In **Stage I** of the pre-training process, we utilized approximately 250 million image-text pairs to train the text-to-image


(T2I) task. In **Stage II**, due to the increased resource demands associated with higher image resolutions, we improved the


quality of images and captions through filtering and recaptioning techniques. Specifically, we employed an aesthetic model


to curate high-quality images, and utilized the Qwen-VL(Wang et al., 2024a) and CogVLM(Wang et al., 2023) models to


generate refined, high-quality captions for these images. Ultimately, we collected and constructed a dataset comprising


approximately 80 million samples for image generation task and 10 million samples for image understanding task.


**B.2. MCoT Data**


To construct the dataset for the planning and acting stage, we utilized Qwen-VL (Wang et al., 2024a) to generate dense


captions. Subsequently, we extracted all object description phrases from the dense captions and employed Grounded SAM


(Ren et al., 2024) to generate the corresponding grounding boxes. For the reflection task, we leveraged the RichHF-18K


dataset (Liang et al., 2024b) and the additional 5,000 images generated by MNIT, which were manually annotated to identify


the bounding boxes of incorrectly generated objects, along with their corresponding prompt contents. For the inpainting


correction task (for correction), we followed the methodology outlined in BrushNet (Ju et al., 2024) to generate random


masks and segmentation masks.


14



