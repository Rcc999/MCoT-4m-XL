# Copyright 2024 EPFL and Apple Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
PyTorch Dataset for MCoT Training with MINT Paper Integration

This dataset handles the JSON format generated by the mcot_dataset_wget.py script.
It includes enhanced features from the MINT paper for better artifact detection
and reflection capabilities.

Key features:
- Handles mixed data sources (MCoT steps + SeeTRUE feedback data)
- Proper tokenization for 4M model training  
- Artifact detection integration for reflection step
- Flexible step-wise training support

The dataset expects JSON data with the following structure:
{
    "image": "path/to/image.jpg",
    "prompt": "user prompt",
    "planning": "planning step output",
    "acting": "acting step output", 
    "reflection": "reflection step output",
    "correction": "correction step output",
    "dataset_source": "mcot" or "seetrue_feedback_as_mint_reflection",
    ... (additional fields for MINT features)
}
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
import torch
from torch.utils.data import Dataset
from PIL import Image
import numpy as np
from fourm.data.modality_info import MODALITY_INFO
from fourm.data.modality_transforms import get_transform_key


class MCoTDataset(Dataset):
    """
    PyTorch Dataset for MCoT training data in JSON format.
    
    This dataset processes the JSON files created by mcot_dataset_wget.py and prepares
    them for 4M model training. It handles image loading, text tokenization, and 
    step-specific data organization.
    
    Features:
    - Automatic image transforms for different input sizes
    - Text tokenization for each MCoT step
    - SeeTRUE feedback integration for enhanced reflection
    - Configurable step subsets for flexible training
    
    Args:
        data: List of JSON objects containing MCoT examples
        modality_info: 4M modality configuration
        input_size: Image input size (default 224)
        num_input_tokens: Max input sequence length
        num_target_tokens: Max target sequence length  
        mcot_steps: MCoT steps to include in training
    """
    
    def __init__(self, 
                 data: List[Dict[str, Any]], 
                 modality_info: Dict[str, Any],
                 input_size: int = 224,
                 num_input_tokens: int = 256, 
                 num_target_tokens: int = 256,
                 mcot_steps: List[str] = ['planning', 'acting', 'reflection', 'correction']):
        
        self.data = data
        self.modality_info = modality_info
        self.input_size = input_size
        self.num_input_tokens = num_input_tokens
        self.num_target_tokens = num_target_tokens
        self.mcot_steps = mcot_steps
        
        # Set up transforms for each modality (image, text, etc.)
        self.transforms = {}
        for mod_name, mod_info in modality_info.items():
            self.transforms[mod_name] = get_transform_key(mod_info, input_size)
    
    def __len__(self) -> int:
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        Get a single MCoT training sample with proper preprocessing.
        
        Loads the image, applies transforms, extracts text for each step,
        and organizes everything into the format expected by 4M training.
        
        Returns:
            Dictionary containing:
            - mod_dict: Modalities for 4M model (images, text for each step)
            - step_texts: Raw text for each MCoT step
            - seetrue_data: Enhanced reflection data from SeeTRUE feedback
            - Various metadata fields
        """
        sample = self.data[idx]
        
        # Load image with fallback to placeholder if file is missing
        image_path = sample.get('image', '')
        if os.path.exists(image_path):
            image = Image.open(image_path).convert('RGB')
        else:
            # Create placeholder image if file missing - prevents training crashes
            image = Image.new('RGB', (self.input_size, self.input_size), color=(128, 128, 128))
        
        # Apply image transforms (resize, normalize, etc.)
        if 'rgb' in self.transforms:
            image_tensor = self.transforms['rgb'](image)
        else:
            # Fallback transform if rgb transform not configured
            from torchvision import transforms
            transform = transforms.Compose([
                transforms.Resize((self.input_size, self.input_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            image_tensor = transform(image)
        
        # Extract text for each MCoT step
        step_texts = {}
        seetrue_data = None
        
        for step in self.mcot_steps:
            step_text = sample.get(step, f"No {step} data available.")
            step_texts[step] = step_text
            
            # Extract enhanced reflection data from SeeTRUE-Feedback for MINT features
            if step == 'reflection' and sample.get('dataset_source') == 'seetrue_feedback_as_mint_reflection':
                seetrue_data = {
                    'bbox_artifacts': sample.get('bbox_artifacts', []),
                    'misalignment_detected': sample.get('misalignment_detected', False),
                    'incorrect_objects': sample.get('incorrect_objects', []),
                    'requires_correction': sample.get('requires_correction', False),
                    'artifact_confidence': sample.get('artifact_confidence', 0.0),
                    'caption_misalignment': sample.get('caption_misalignment', ''),
                    'visual_misalignment': sample.get('visual_misalignment', ''),
                    'human_feedback': sample.get('human_feedback', [])
                }
        
        # Create modality dictionary compatible with 4M model training
        mod_dict = {
            'rgb': image_tensor.unsqueeze(0),  # [1, C, H, W]
        }
        
        # Add text modality for each MCoT step
        for step in self.mcot_steps:
            mod_dict[f'text_{step}'] = step_texts[step]
        
        # Add input prompt as primary text modality
        mod_dict['text_input'] = sample.get('prompt', 'Generate image')
        
        # Set target text (final result or correction step)
        target_text = sample.get('final_response', step_texts.get('correction', 'Complete MCoT process.'))
        
        return {
            'mod_dict': mod_dict,
            'image_id': sample.get('image_id', f'sample_{idx}'),
            'mcot_step': sample.get('mcot_step', 'planning'),
            'step_texts': step_texts,
            'target_text': target_text,
            'seetrue_data': seetrue_data,  # Enhanced reflection data from SeeTRUE-Feedback
            'num_input_tokens': self.num_input_tokens,
            'num_target_tokens': self.num_target_tokens
        }
    
    def collate_fn(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Collate function for batching MCoT samples.
        
        Args:
            batch: List of samples from __getitem__
            
        Returns:
            Batched data compatible with 4M model training
        """
        batch_size = len(batch)
        
        # Collect all modalities
        mod_dict = {}
        
        # Batch RGB images
        if 'rgb' in batch[0]['mod_dict']:
            rgb_tensors = [sample['mod_dict']['rgb'].squeeze(0) for sample in batch]
            mod_dict['rgb'] = torch.stack(rgb_tensors, dim=0)
        
        # Batch text modalities
        text_modalities = ['text_input'] + [f'text_{step}' for step in self.mcot_steps]
        for text_mod in text_modalities:
            if text_mod in batch[0]['mod_dict']:
                mod_dict[text_mod] = [sample['mod_dict'][text_mod] for sample in batch]
        
        # Collect step-specific data
        step_data = {}
        for step in self.mcot_steps:
            step_data[step] = {
                'texts': [sample['step_texts'].get(step, '') for sample in batch],
                'mod_dict': {
                    'rgb': mod_dict.get('rgb'),
                    f'text_{step}': mod_dict.get(f'text_{step}', [''] * batch_size)
                }
            }
            
            # Add SeeTRUE-Feedback data for reflection step
            if step == 'reflection':
                seetrue_batch = []
                for sample in batch:
                    seetrue_data = sample.get('seetrue_data')
                    seetrue_batch.append(seetrue_data if seetrue_data else None)
                step_data[step]['seetrue_data'] = seetrue_batch
        
        # Collect metadata
        image_ids = [sample['image_id'] for sample in batch]
        mcot_steps = [sample['mcot_step'] for sample in batch]
        target_texts = [sample['target_text'] for sample in batch]
        
        return {
            'mod_dict': mod_dict,
            'image_ids': image_ids,
            'mcot_steps': mcot_steps,
            'target_texts': target_texts,
            'step_data': step_data,
            'num_input_tokens': batch[0]['num_input_tokens'],
            'num_target_tokens': batch[0]['num_target_tokens'],
            'batch_size': batch_size
        }


class MCoTDatasetFromWgetOutput(Dataset):
    """
    PyTorch Dataset that loads data from wget script output directory structure.
    Handles the directory/JSON format created by mcot_dataset_wget.py.
    """
    
    def __init__(self, 
                 dataset_dir: str,
                 modality_info: Dict[str, Any],
                 input_size: int = 224,
                 num_input_tokens: int = 256,
                 num_target_tokens: int = 256,
                 mcot_steps: List[str] = ['planning', 'acting', 'reflection', 'correction'],
                 split: str = 'train'):
        
        self.dataset_dir = Path(dataset_dir)
        self.modality_info = modality_info
        self.input_size = input_size
        self.num_input_tokens = num_input_tokens
        self.num_target_tokens = num_target_tokens
        self.mcot_steps = mcot_steps
        self.split = split
        
        # Find all example directories
        split_dir = self.dataset_dir / split
        if not split_dir.exists():
            raise ValueError(f"Split directory {split_dir} does not exist")
        
        self.example_dirs = []
        for example_dir in split_dir.iterdir():
            if example_dir.is_dir() and (example_dir / "mcot_annotations.json").exists():
                self.example_dirs.append(example_dir)
        
        if not self.example_dirs:
            raise ValueError(f"No valid examples found in {split_dir}")
        
        print(f"Found {len(self.example_dirs)} examples in {split} split")
        
        # Setup transforms
        self.transforms = {}
        for mod_name, mod_info in modality_info.items():
            self.transforms[mod_name] = get_transform_key(mod_info, input_size)
    
    def __len__(self) -> int:
        return len(self.example_dirs)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """Load example from wget output directory structure."""
        example_dir = self.example_dirs[idx]
        
        # Load annotations
        annotations_file = example_dir / "mcot_annotations.json"
        with open(annotations_file, 'r') as f:
            annotations = json.load(f)
        
        # Load image
        image_file = example_dir / "image.jpg"
        if image_file.exists():
            image = Image.open(image_file).convert('RGB')
        else:
            # Create placeholder image
            image = Image.new('RGB', (self.input_size, self.input_size), color=(128, 128, 128))
        
        # Apply image transforms  
        if 'rgb' in self.transforms:
            image_tensor = self.transforms['rgb'](image)
        else:
            # Fallback transform
            from torchvision import transforms
            transform = transforms.Compose([
                transforms.Resize((self.input_size, self.input_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            image_tensor = transform(image)
        
        # Extract step texts
        step_texts = {}
        for step in self.mcot_steps:
            step_texts[step] = annotations.get(step, f"No {step} data available.")
        
        # Create modality dictionary
        mod_dict = {
            'rgb': image_tensor.unsqueeze(0),
            'text_input': annotations.get('prompt', 'Generate image')
        }
        
        # Add step-specific text modalities
        for step in self.mcot_steps:
            mod_dict[f'text_{step}'] = step_texts[step]
        
        return {
            'mod_dict': mod_dict,
            'image_id': annotations.get('image_id', example_dir.name),
            'mcot_step': 'planning',  # Default step
            'step_texts': step_texts,
            'target_text': annotations.get('final_response', step_texts.get('correction', 'Complete MCoT process.')),
            'num_input_tokens': self.num_input_tokens,
            'num_target_tokens': self.num_target_tokens
        }
    
    def collate_fn(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Same collate function as MCoTDataset."""
        return MCoTDataset.collate_fn(self, batch)