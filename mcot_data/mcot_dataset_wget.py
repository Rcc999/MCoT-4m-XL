import csv
import json
import os
import tarfile
import math
import subprocess
import shutil
import random
import requests
import zipfile
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Optional, Tuple, Union, Any

import datasets
import tensorflow as tf
import pandas as pd
import torch
import numpy as np
from PIL import Image, ImageDraw
import webdataset as wds
import argparse


_CITATION = """
@article{wang2024mint,
  title={MINT: Multi-modal Chain of Thought in Unified Generative Models for Enhanced Image Generation},
  author={Wang, Yi and Liu, Mushui and He, Wanggui and Zhang, Longxiang and Huang, Ziwei and Zhang, Guanghao and Shu, Fangxun and Tao, Zhong and She, Dong and Yu, Zhelun and Li, Haoyuan and Dai, Weilong and Song, Mingli and Song, Jie and Jiang, Hao},
  journal={arXiv preprint arXiv:2503.01298},
  year={2024}
}
"""

_DESCRIPTION = """
Multimodal Chain of Thought (MCoT) dataset combining data from actplan (replacing Visual Genome), RichHF-18K,
SeeTRUE-Feedback, and BrushData to create a step-by-step multimodal reasoning pipeline.
"""

_URLS = {
    "actplan": {
        "local_json_file": "actplan.json"  # Located in same directory as this script
    },
    "richhf18k": {
        "tfrecord_urls": {
            "train": "https://github.com/google-research-datasets/richhf-18k/raw/main/train.tfrecord",
            "dev": "https://github.com/google-research-datasets/richhf-18k/raw/main/dev.tfrecord", 
            "test": "https://github.com/google-research-datasets/richhf-18k/raw/main/test.tfrecord"
        }
    },
    "seetrue_feedback": {
        "hf_dataset_name": "mismatch-quest/SeeTRUE-Feedback"
    },
    "brush_data": {
        "base_tar_url": "https://huggingface.co/datasets/random123123/BrushData/resolve/main/",
        "tar_files": [
            "00000.tar", "00001.tar", "00002.tar", "00003.tar", "00004.tar",
            "00005.tar", "00006.tar"
        ]
    }
}


class MCoTWgetDataset(datasets.GeneratorBasedBuilder):
    VERSION = datasets.Version("1.1.2") # Incremented version due to cleanup feature

    BUILDER_CONFIGS = [
        datasets.BuilderConfig(name="default", version=VERSION, description="Default MCoT dataset configuration with cleanup")
    ]

    DEFAULT_CONFIG_NAME = "default"

    def _info(self):
        features = datasets.Features(
            {
                "image_id": datasets.Value("string"),
                "image": datasets.Image(),
                "prompt": datasets.Value("string"),
                "planning": datasets.Value("string"),
                "acting": datasets.Value("string"),
                "reflection": datasets.Value("string"),
                "correction": datasets.Value("string"),
                "final_response": datasets.Value("string")
            }
        )
        return datasets.DatasetInfo(
            description=_DESCRIPTION,
            features=features,
            citation=_CITATION,
        )

    def _download_all_datasets_first(self, dl_manager, base_data_dir):
        """Download all datasets first using wget, then process from cache"""
        print("üöÄ Step 1: Downloading all datasets first...")
        
        # Create cache directories
        seetrue_cache_dir = base_data_dir / "seetrue_feedback_raw" / "hf_cache"
        
        # Download SeeTRUE-Feedback
        print(f"üì¶ Downloading SeeTRUE-Feedback dataset...")
        dataset = datasets.load_dataset(
            _URLS["seetrue_feedback"]["hf_dataset_name"],
            split="test",
            cache_dir=str(seetrue_cache_dir),
            trust_remote_code=True
        )
        print(f"‚úÖ SeeTRUE-Feedback downloaded successfully")
        
        print("üéâ All dataset downloads completed! Now processing...")

    def _split_generators(self, dl_manager):
        user_manual_dir = dl_manager.manual_dir
        base_data_dir = Path(user_manual_dir) if user_manual_dir else Path(os.getcwd()) / "mcot_downloads"
        base_data_dir.mkdir(parents=True, exist_ok=True)

        # Download all datasets first
        self._download_all_datasets_first(dl_manager, base_data_dir)

        processed_data_dir = base_data_dir / "processed_mcot_steps"
        processed_data_dir.mkdir(parents=True, exist_ok=True)

        richhf_raw_dir = base_data_dir / "richhf18k_raw"
        seetrue_raw_dir = base_data_dir / "seetrue_feedback_raw"
        brush_raw_dir = base_data_dir / "brush_data_raw"

        print("üîÑ Step 2: Processing downloaded datasets...")
        # Process actplan data for planning step
        self._process_actplan(processed_data_dir)

        # Process RichHF-18K for reflection training
        self._download_and_prepare_richhf18k(dl_manager, richhf_raw_dir, _URLS["richhf18k"])
        self._process_richhf18k(richhf_raw_dir, processed_data_dir)

        # Process SeeTRUE-Feedback for reflection training  
        self._download_and_prepare_seetrue_feedback(dl_manager, seetrue_raw_dir, _URLS["seetrue_feedback"])
        self._process_seetrue_feedback(seetrue_raw_dir, processed_data_dir)

        # Process BrushData for correction step
        self._download_and_prepare_brush_data(dl_manager, brush_raw_dir, _URLS["brush_data"])
        self._process_brush_data(brush_raw_dir, processed_data_dir)

        self._generate_mcot_examples(processed_data_dir)
        
        # Cleanup raw and intermediate files
        print("MCoT dataset construction complete. Starting cleanup of raw and intermediate files...")
        
        # 1. Delete Raw Data Directories
        raw_data_paths_to_clean = [richhf_raw_dir, seetrue_raw_dir, brush_raw_dir]
        for raw_path in raw_data_paths_to_clean:
            if raw_path.exists():
                print(f"Cleaning up raw data directory: {raw_path}")
                shutil.rmtree(raw_path)
                print(f"Successfully removed {raw_path}")

        # 2. Delete Intermediate Processed JSON Files
        intermediate_json_files = [
            processed_data_dir / "planning_data.json",
            processed_data_dir / "richhf_reflection_data.json",
            processed_data_dir / "seetrue_reflection_data.json", 
            processed_data_dir / "correction_data.json"
        ]
        for json_file in intermediate_json_files:
            if json_file.exists():
                print(f"Cleaning up intermediate file: {json_file}")
                json_file.unlink()
                print(f"Successfully removed {json_file}")
        
        print("Cleanup process finished.")

        return [
            datasets.SplitGenerator(
                name=datasets.Split.TRAIN,
                gen_kwargs={"data_dir": processed_data_dir / "train"},
            ),
            datasets.SplitGenerator(
                name=datasets.Split.VALIDATION,
                gen_kwargs={"data_dir": processed_data_dir / "val"},
            ),
        ]




    def _download_and_prepare_richhf18k(self, dl_manager, richhf_raw_dir, urls):
        """Download RichHF-18K TFRecord files using direct wget"""
        richhf_raw_dir.mkdir(parents=True, exist_ok=True)
        
        print("üéØ Downloading RichHF-18K TFRecord files using direct wget...")
        
        tfrecord_urls = urls.get("tfrecord_urls", {})
        
        for split, url in tfrecord_urls.items():
            filename = f"{split}.tfrecord"
            target_path = richhf_raw_dir / filename
            
            if target_path.exists():
                file_size = target_path.stat().st_size
                if file_size > 1024:  # File is larger than 1KB
                    print(f"‚úÖ {filename} already exists ({file_size:,} bytes)")
                    continue
                else:
                    print(f"‚ö†Ô∏è {filename} exists but is too small ({file_size} bytes). Re-downloading...")
            
            try:
                print(f"üì• Downloading {filename} from {url}...")
                result = subprocess.run(
                    ["wget", "-O", str(target_path), url],
                    check=True
                )
                
                # Verify the downloaded file
                if target_path.exists():
                    file_size = target_path.stat().st_size
                    if file_size > 1024:  # Verify it's not a placeholder
                        print(f"‚úÖ Successfully downloaded {filename} ({file_size:,} bytes)")
                    else:
                        print(f"‚ö†Ô∏è Downloaded {filename} but file seems too small ({file_size} bytes)")
                else:
                    print(f"‚ùå Failed to download {filename} - file not found after wget")
                    
            except subprocess.CalledProcessError as e:
                print(f"‚ùå Failed to download {filename}: {e}")
                raise RuntimeError(f"Failed to download {filename}")
            except Exception as e:
                print(f"‚ùå Unexpected error downloading {filename}: {e}")
                raise
        
        print("‚úÖ RichHF-18K TFRecord download process completed.")


    def _download_and_prepare_seetrue_feedback(self, dl_manager, seetrue_raw_dir, urls):
        """Download SeeTRUE-Feedback dataset using HuggingFace datasets"""
        seetrue_raw_dir.mkdir(parents=True, exist_ok=True)
        cache_dir = seetrue_raw_dir / "hf_cache"
        cache_dir.mkdir(exist_ok=True, parents=True)
        
        print(f"üì¶ Downloading SeeTRUE-Feedback dataset...")
        
        try:
            dataset = datasets.load_dataset(
                urls["hf_dataset_name"],
                split="test",
                cache_dir=str(cache_dir),
                trust_remote_code=True
            )
            
            # Save dataset to JSON for processing
            dataset_path = seetrue_raw_dir / "seetrue_data.json"
            dataset.to_json(str(dataset_path))
            print(f"‚úÖ SeeTRUE-Feedback dataset downloaded successfully ({len(dataset)} samples)")
            
            # Create success flag
            (seetrue_raw_dir / "seetrue_download_completed.flag").touch()
            
        except Exception as e:
            print(f"‚ùå Could not download SeeTRUE-Feedback dataset: {e}")
            raise


    def _download_and_prepare_brush_data(self, dl_manager, brush_raw_dir, urls):
        """Download BrushData using direct wget - limited to 20GB instead of 1.7TB"""
        brush_raw_dir.mkdir(parents=True, exist_ok=True)
        tars_dir = brush_raw_dir / "tars"
        tars_dir.mkdir(exist_ok=True)
        
        print("üéØ Downloading BrushData using direct wget (limited to 20GB)...")
        
        base_url = urls.get("base_tar_url", "")
        tar_files = urls.get("tar_files", [])
        
        if not base_url or not tar_files:
            raise ValueError("No direct URLs configured for BrushData")
        
        downloaded_files = []
        total_size_gb = 0
        MAX_SIZE_GB = 20
        
        for i, tar_filename in enumerate(tar_files):
            if total_size_gb >= MAX_SIZE_GB:
                print(f"üõë Reached 20GB limit, stopping BrushData download")
                break
                
            tar_url = base_url + tar_filename
            tar_local_path = tars_dir / tar_filename
            
            if tar_local_path.exists():
                file_size_gb = tar_local_path.stat().st_size / (1024**3)
                print(f"üìÅ {tar_filename} already exists ({file_size_gb:.1f}GB)")
                downloaded_files.append(tar_local_path)
                total_size_gb += file_size_gb
                continue
            
            try:
                print(f"üì¶ Downloading BrushData {tar_filename} ({i+1}/{len(tar_files)})...")
                downloaded_path = dl_manager.download_and_extract(tar_url)
                
                # Copy to our tars directory
                if Path(downloaded_path).is_file():
                    shutil.copy(downloaded_path, tar_local_path)
                    file_size_gb = tar_local_path.stat().st_size / (1024**3)
                    downloaded_files.append(tar_local_path)
                    total_size_gb += file_size_gb
                    print(f"‚úÖ {tar_filename} downloaded ({file_size_gb:.1f}GB, total: {total_size_gb:.1f}GB)")
                else:
                    print(f"‚ö†Ô∏è {tar_filename} download failed")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to download {tar_filename}: {e}")
                continue
        
        print(f"‚úÖ BrushData direct download completed! Downloaded {len(downloaded_files)} TAR files ({total_size_gb:.1f}GB)")
        (brush_raw_dir / "brush_direct_download_completed.flag").touch()
        
        # Save list of downloaded files for processing
        with open(brush_raw_dir / "downloaded_tars.json", 'w') as f:
            json.dump([str(p) for p in downloaded_files], f)

    def _process_actplan(self, processed_data_dir):
        """Process actplan dataset that has short captions, dense captions, and bounding boxes"""
        planning_samples = []
        
        # Load actplan.json from the mcot_data directory
        actplan_json_path = Path(__file__).parent / "actplan.json"
        
        if not actplan_json_path.exists():
            raise FileNotFoundError(f"actplan.json not found at {actplan_json_path}")
        
        print(f"üìä Loading actplan dataset from {actplan_json_path}...")
        
        try:
            with open(actplan_json_path, 'r') as f:
                actplan_data = json.load(f)
            
            print(f"‚úÖ Loaded {len(actplan_data)} entries from actplan dataset")
            
            # Process each actplan entry
            processed_count = 0
            
            for entry in actplan_data:
                image_id = entry.get("image_id", f"actplan_{processed_count}")
                original_captions = entry.get("original_captions", [])
                dense_captions = entry.get("dense_captions", [])
                bounding_boxes = entry.get("bounding_boxes", [])
                
                # Use the first original caption as the prompt for acting step
                prompt = original_captions[0] if original_captions else f"Generate image for {image_id}"
                
                # Use the first dense caption as the main planning description
                dense_caption = dense_captions[0] if dense_captions else "No dense caption available"
                
                # Create planning text following MINT methodology
                planning_text = f"Planning: Dense scene analysis based on detailed captioning. "
                planning_text += f"Dense caption: {dense_caption}. "
                
                # Add bounding box information for spatial planning
                if bounding_boxes:
                    bbox_descriptions = []
                    for bbox in bounding_boxes:
                        obj_class = bbox.get("class", "object")
                        x1, y1, x2, y2 = bbox.get("x1", 0), bbox.get("y1", 0), bbox.get("x2", 0), bbox.get("y2", 0)
                        bbox_descriptions.append(f"{obj_class} (x1:{x1}, y1:{y1}, x2:{x2}, y2:{y2})")
                    
                    planning_text += f"Spatial layout with bounding boxes: {'; '.join(bbox_descriptions)}. "
                
                # Add alternative captions for variation
                if len(original_captions) > 1:
                    alt_captions = "; ".join(original_captions[1:3])  # Use 2nd and 3rd captions
                    planning_text += f"Alternative descriptions: {alt_captions}. "
                
                if len(dense_captions) > 1:
                    alt_dense = dense_captions[1]  # Use 2nd dense caption
                    planning_text += f"Alternative dense description: {alt_dense}. "
                
                planning_text += "Actplan structured captioning and spatial analysis complete."
                
                # Create planning sample with proper prompt field for acting step
                planning_sample = {
                    "image_id": image_id,
                    "prompt": prompt,  # This is crucial - provides proper prompt for acting step
                    "planning": planning_text,
                    "image": None,  # No images in actplan dataset
                    "original_captions": original_captions,
                    "dense_captions": dense_captions,
                    "bounding_boxes": bounding_boxes
                }
                
                planning_samples.append(planning_sample)
                processed_count += 1
                
                if processed_count % 500 == 0:
                    print(f"üìà Processed {processed_count} actplan samples...")
            
            print(f"‚úÖ Processed {len(planning_samples)} actplan planning samples")
            
            # Save processed data
            with open(processed_data_dir / "planning_data.json", 'w') as f:
                serializable_samples = []
                for ps in planning_samples:
                    s_ps = ps.copy()
                    # Remove image field for JSON serialization (it's None anyway)
                    if "image" in s_ps:
                        s_ps.pop("image")
                    serializable_samples.append(s_ps)
                json.dump(serializable_samples, f, indent=2)
            
            self.actplan_planning_samples_with_images = planning_samples
            
        except Exception as e:
            print(f"‚ùå Error processing actplan dataset: {e}")
            raise


    def _process_richhf18k(self, richhf_raw_dir, processed_data_dir):
        """
        Process RichHF-18K dataset for reflection task training.
        
        MODIFIED: Now uses the FULL RichHF-18K dataset instead of limiting to 1000 samples per file.
        This aligns with the MINT paper methodology which leverages the complete RichHF-18K dataset
        for reflection and artifact identification training.
        """
        reflection_samples = []
        
        # MINT paper: "For the reflection task, we leveraged the RichHF-18K dataset and the additional 5,000 images 
        # generated by MINT, which were manually annotated to identify the bounding boxes of incorrectly generated 
        # objects, along with their corresponding prompt contents."
        # RichHF-18K provides human feedback for reflection and artifact identification
        
        # Look for TFRecord files downloaded directly via wget
        # Process directly downloaded TFRecord files
        # Use full RichHF-18K dataset as per MINT paper methodology
        print("üéØ Using FULL RichHF-18K dataset (no sample limits)")
        
        # Look for TFRecord files in the direct download location
        tfrecord_files = list(richhf_raw_dir.glob("*.tfrecord"))
        
        print(f"Found {len(tfrecord_files)} TFRecord files in RichHF-18K directory")
        
        # Process TFRecord files (preferred format)
        if tfrecord_files:
            print(f"üîÑ Processing {len(tfrecord_files)} TFRecord files...")
            for tfrecord_file in tfrecord_files:
                print(f"üìÇ Processing TFRecord: {tfrecord_file}")
                
                # Check if file is a valid TFRecord
                file_size = tfrecord_file.stat().st_size
                if file_size < 1000:  # Likely too small to be valid
                    raise ValueError(f"{tfrecord_file.name} appears to be too small (size: {file_size} bytes) - download failure")
                
                dataset = tf.data.TFRecordDataset(str(tfrecord_file))
                processed_count = 0
                skipped_count = 0
                total_records = 0
                
                # Process ALL records in the TFRecord file (no limit)
                for i, raw_record in enumerate(dataset):
                    total_records += 1
                    example = tf.train.Example()
                    example.ParseFromString(raw_record.numpy())
                    features = example.features.feature
                    
                    # Extract TFRecord data using ACTUAL RichHF-18K schema
                    filename = self._get_string_feature(features, 'filename')
                    artifact_score = self._get_float_feature(features, 'artifact_score') 
                    misalignment_score = self._get_float_feature(features, 'misalignment_score')
                    overall_score = self._get_float_feature(features, 'overall_score')
                    aesthetics_score = self._get_float_feature(features, 'aesthetics_score')
                    prompt_misalignment_label = self._get_string_feature(features, 'prompt_misalignment_label')
                    
                    # Generate prompt from filename (remove path and extension)
                    if filename:
                        # Extract base filename and convert to a reasonable prompt
                        base_filename = filename.split('/')[-1].replace('.png', '').replace('.jpg', '')
                        prompt = f"Generate image: {base_filename}"
                    else:
                        prompt = "Generate image based on feedback data"
                    
                    if prompt:
                        reflection_sample = self._process_richhf_reflection_sample({
                            "prompt": prompt,
                            "artifact_score": artifact_score,
                            "misalignment_score": misalignment_score,
                            "overall_score": overall_score,
                            "aesthetics_score": aesthetics_score,
                            "prompt_misalignment_label": prompt_misalignment_label,
                            "feedback": f"Artifact score: {artifact_score:.3f}, Alignment score: {misalignment_score:.3f}, Overall quality: {overall_score:.3f}, Aesthetics: {aesthetics_score:.3f}",
                            "feedback_score": overall_score,
                            "quality_score": aesthetics_score, 
                            "aspect_scores": [artifact_score, misalignment_score, overall_score, aesthetics_score]
                        }, f"richhf_{tfrecord_file.stem}_{i}")
                        if reflection_sample:
                            reflection_samples.append(reflection_sample)
                            processed_count += 1
                        else:
                            skipped_count += 1
                        
                success_rate = (processed_count / total_records * 100) if total_records > 0 else 0
                print(f"‚úÖ RichHF TFRecord Summary: {processed_count}/{total_records} samples processed successfully ({success_rate:.1f}%), {skipped_count} samples skipped from {tfrecord_file.name}")
                        
        print(f"‚úÖ RichHF-18K Processing Complete: {len(reflection_samples)} total reflection samples successfully processed")

        with open(processed_data_dir / "richhf_reflection_data.json", 'w') as f:
            json.dump(reflection_samples, f)
        self.richhf_reflection_samples = reflection_samples

    def _process_richhf_reflection_sample(self, data, image_id):
        """Process individual RichHF-18K sample for reflection task following MINT methodology"""
        prompt = data.get("prompt", data.get("text", data.get("caption", "")))
        
        # Extract artifact and quality information for reflection analysis
        artifact_score = data.get("artifact_score", 0.0)
        misalignment_score = data.get("misalignment_score", 0.0)
        overall_score = data.get("overall_score", 0.0)
        aesthetics_score = data.get("aesthetics_score", 0.0)
        prompt_misalignment_label = data.get("prompt_misalignment_label", "")
        
        # Extract feedback information
        feedback = data.get("feedback", data.get("human_feedback", ""))
        quality_score = data.get("quality_score", data.get("quality", data.get("score", data.get("rating", 0.5))))
        feedback_score = data.get("feedback_score", 0.0)
        aspect_scores = data.get("aspect_scores", [])
        
        if not prompt:
            print(f"‚ö†Ô∏è Skipping RichHF sample {image_id}: Missing prompt data")
            return None
            
        # Generate reflection text following MINT paper specifications for artifact identification
        reflection_text = "Reflection: Analyzing generated image for artifacts and misalignments. "
        reflection_text += f"Original prompt: '{prompt}' "
        
        # Focus on artifact detection and bounding box identification (core MINT reflection task)
        if artifact_score > 0.3:  # Threshold for significant artifacts
            reflection_text += f"Artifact detection: High artifact presence (score: {artifact_score:.3f}). "
            reflection_text += "Identifying bounding boxes of incorrectly generated objects. "
            
        if misalignment_score > 0.3:  # Threshold for significant misalignment
            reflection_text += f"Prompt misalignment detected (score: {misalignment_score:.3f}). "
            if prompt_misalignment_label:
                reflection_text += f"Misalignment type: {prompt_misalignment_label}. "
            reflection_text += "Analyzing correspondence between prompt content and visual elements. "
            
        if overall_score < 0.6:  # Low quality threshold
            reflection_text += f"Quality assessment: Below threshold (score: {overall_score:.3f}). "
            reflection_text += "Identifying specific regions requiring correction. "
            
        if feedback:
            feedback_clean = feedback[:200] if len(feedback) > 200 else feedback
            reflection_text += f"Human feedback analysis: {feedback_clean} "
            
        # Add MINT-specific reflection components
        reflection_text += "Generating artifact heatmap for targeted correction. "
        reflection_text += "Self-reflection on generation accuracy and aesthetic quality. "
        reflection_text += "Preparing bounding box annotations for incorrectly generated objects."
        
        return {
            "image_id": str(image_id),
            "prompt": prompt,
            "reflection": reflection_text,
            "artifact_score": artifact_score,
            "misalignment_score": misalignment_score,
            "overall_score": overall_score,
            "aesthetics_score": aesthetics_score,
            "prompt_misalignment_label": prompt_misalignment_label,
            "quality_score": quality_score,
            "feedback": feedback,
            "feedback_score": feedback_score,
            "aspect_scores": aspect_scores,
            # MINT-specific fields for reflection task
            "requires_artifact_correction": artifact_score > 0.3,
            "requires_alignment_correction": misalignment_score > 0.3,
            "requires_quality_improvement": overall_score < 0.6
        }
    
    def _generate_mint_acting_text(self, prompt):
        """Generate acting text following MINT paper methodology"""
        acting_text = "Acting: Generate the image based on the planning outputs. "
        acting_text += f"Use the caption and layout information to create a coherent visual representation. "
        acting_text += f"Prompt guidance: '{prompt}' "
        acting_text += "Follow the spatial relationships and object placements from the planning step. "
        acting_text += "Execute with attention to fine-grained alignment and interwoven conditions. "
        acting_text += "Maintain consistency with dense caption and bounding box specifications."
        return acting_text

    def _get_string_feature(self, features, key):
        return features[key].bytes_list.value[0].decode('utf-8') if key in features and features[key].bytes_list.value else ""

    def _get_float_feature(self, features, key):
        return features[key].float_list.value[0] if key in features and features[key].float_list.value else 0.0

    def _get_float_list_feature(self, features, key):
        return list(features[key].float_list.value) if key in features else []

    def _process_seetrue_feedback(self, seetrue_raw_dir, processed_data_dir):
        """
        Process SeeTRUE-Feedback as a replacement for MINT's manually annotated reflection data.
        
        MINT paper: "For the reflection task, we leveraged the RichHF-18K dataset and the additional 
        5,000 images generated by MINT, which were manually annotated to identify the bounding boxes 
        of incorrectly generated objects, along with their corresponding prompt contents."
        
        SeeTRUE-Feedback provides equivalent data with rich misalignment annotations and bounding boxes.
        """
        reflection_samples = []
        
        try:
            print("üîÑ Loading SeeTRUE-Feedback as MINT reflection data replacement")
            
            # First try to load from JSON file if it exists
            seetrue_data_file = seetrue_raw_dir / "seetrue_data.json"
            if seetrue_data_file.exists():
                print(f"üìÅ Loading SeeTRUE data from cached JSONL file...")
                # SeeTRUE data is saved in JSONL format by HuggingFace datasets.to_json()
                seetrue_data = []
                processed_count = 0
                skipped_count = 0
                total_lines = 0
                with open(seetrue_data_file, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        total_lines += 1
                        try:
                            # Parse each line as a separate JSON object (JSONL format)
                            sample = json.loads(line.strip())
                            
                            # Process SeeTRUE-Feedback following MINT paper methodology
                            processed_sample = self._process_seetrue_as_mint_reflection(sample, i)
                            if processed_sample:
                                reflection_samples.append(processed_sample)
                                processed_count += 1
                                
                                if processed_count % 1000 == 0:
                                    print(f"   ‚úÖ Processed {processed_count} SeeTRUE samples for reflection training")
                            else:
                                skipped_count += 1
                        except json.JSONDecodeError as e:
                            skipped_count += 1
                            print(f"‚ö†Ô∏è Skipping malformed JSON line {i}: {e}")
                            
                success_rate = (processed_count / total_lines * 100) if total_lines > 0 else 0
                print(f"‚úÖ JSONL Processing Summary: {processed_count}/{total_lines} samples processed successfully ({success_rate:.1f}%), {skipped_count} samples skipped")
                
            else:
                # Try to load directly using datasets library
                try:
                    if datasets is not None:
                        seetrue_hf_dataset = datasets.load_dataset(
                            _URLS["seetrue_feedback"]["hf_dataset_name"],
                            split="test",  # Use 'test' split instead of 'train'
                            cache_dir=str(seetrue_raw_dir / "hf_cache"),
                            trust_remote_code=True
                        )
                        
                        processed_count = 0
                        skipped_count = 0
                        for i, sample in enumerate(seetrue_hf_dataset):
                            # Process SeeTRUE-Feedback following MINT paper methodology
                            processed_sample = self._process_seetrue_as_mint_reflection(sample, i)
                            if processed_sample:
                                reflection_samples.append(processed_sample)
                                processed_count += 1
                                
                                if processed_count % 1000 == 0:
                                    print(f"   ‚úÖ Processed {processed_count} SeeTRUE samples for reflection training")
                            else:
                                skipped_count += 1
                                
                        total_samples = len(seetrue_hf_dataset)
                        success_rate = (processed_count / total_samples * 100) if total_samples > 0 else 0
                        print(f"‚úÖ SeeTRUE Processing Summary: {processed_count}/{total_samples} samples processed successfully ({success_rate:.1f}%), {skipped_count} samples skipped")
                    else:
                        raise ImportError("datasets library not available")
                        
                except Exception as hf_error:
                    raise RuntimeError(f"Could not load SeeTRUE-Feedback from HuggingFace: {hf_error}")
                    
        except Exception as e:
            raise RuntimeError(f"Error processing SeeTRUE-Feedback dataset: {e}")
        
        print(f"‚úÖ SeeTRUE-Feedback Processing Complete: {len(reflection_samples)} total reflection samples successfully processed")
        
        with open(processed_data_dir / "seetrue_reflection_data.json", 'w') as f:
            json.dump(reflection_samples, f)
        self.seetrue_reflection_samples = reflection_samples

    def _process_seetrue_as_mint_reflection(self, sample, sample_id):
        """
        Process SeeTRUE-Feedback sample into MINT reflection format using actual SeeTRUE data structure.
        
        Args:
            sample: SeeTRUE-Feedback sample with actual format:
                   - image_caption: string
                   - human_feedback: string  
                   - caption_misalignment: string
                   - visual_misalignment: string
                   - bbox_GroundingDino: string (JSON-like)
                   - bbox_PaLI: string (bbox coordinates)
            sample_id: Unique identifier for the sample
            
        Returns:
            Dict containing MINT-formatted reflection data, or None if processing fails
        """
        try:
            # Extract core data from actual SeeTRUE sample format
            prompt = sample.get("image_caption", "")
            image = sample.get("image")
            
            # Extract human feedback and misalignment data
            human_feedback = sample.get("human_feedback", [])
            caption_misalignment = sample.get("caption_misalignment", "")
            visual_misalignment = sample.get("visual_misalignment", "")
            
            # Extract bounding box data (these are strings in the actual dataset)
            bbox_grounding_dino_str = sample.get("bbox_GroundingDino", "")
            bbox_pali_str = sample.get("bbox_PaLI", "")
            
            if not prompt or not image:
                missing_fields = []
                if not prompt:
                    missing_fields.append("prompt")
                if not image:
                    missing_fields.append("image")
                print(f"‚ö†Ô∏è Skipping SeeTRUE sample {sample_id}: Missing required fields: {', '.join(missing_fields)}")
                return None
                
            # Generate MINT-style reflection text using actual SeeTRUE data
            reflection_text = "Reflection: Analyzing generated image for artifacts and misalignments using SeeTRUE-Feedback methodology. "
            reflection_text += f"Original prompt: '{prompt}' "
            
            # Process misalignment detection
            misalignment_detected = False
            if caption_misalignment and visual_misalignment:
                misalignment_detected = True
                reflection_text += f"Misalignment detected: Expected '{caption_misalignment}' but found '{visual_misalignment}'. "
            
            # Process human feedback
            if isinstance(human_feedback, list) and human_feedback:
                feedback_summary = "; ".join(human_feedback[:2])  # Take first 2 feedback items
                reflection_text += f"Human feedback: {feedback_summary}. "
            elif isinstance(human_feedback, str) and human_feedback:
                reflection_text += f"Human feedback: {human_feedback[:150]}. "
            
            # Parse and process GroundingDino bounding boxes
            detected_artifacts = []
            grounding_dino_objects = []
            if bbox_grounding_dino_str:
                try:
                    # Parse the GroundingDino bbox string (it's JSON-like)
                    import ast
                    bbox_data = ast.literal_eval(bbox_grounding_dino_str)
                    if isinstance(bbox_data, dict):
                        boxes = bbox_data.get("boxes", [])
                        labels = bbox_data.get("labels", [])
                        
                        for i, (box, label) in enumerate(zip(boxes, labels)):
                            # Extract confidence from label if available (format: 'object(confidence)')
                            confidence = 0.8  # Default confidence
                            clean_label = label
                            if '(' in label and ')' in label:
                                try:
                                    clean_label = label.split('(')[0]
                                    confidence_str = label.split('(')[1].split(')')[0]
                                    confidence = float(confidence_str)
                                except:
                                    pass
                            
                            grounding_dino_objects.append({
                                "bbox": box,
                                "confidence": confidence,
                                "label": clean_label
                            })
                            
                            # Add to detected artifacts if confidence is high
                            if confidence > 0.5:
                                detected_artifacts.append({
                                    "bbox": box,
                                    "confidence": confidence,
                                    "label": clean_label,
                                    "source": "GroundingDino"
                                })
                        
                        if grounding_dino_objects:
                            reflection_text += f"GroundingDino detected {len(grounding_dino_objects)} objects. "
                            
                except Exception as e:
                    print(f"Warning: Failed to parse GroundingDino bbox data: {e}")
            
            # Parse and process PaLI bounding boxes (simpler format: "x1 y1 x2 y2 label")
            pali_objects = []
            if bbox_pali_str:
                try:
                    parts = bbox_pali_str.strip().split()
                    if len(parts) >= 5:  # x1 y1 x2 y2 label1 label2...
                        x1, y1, x2, y2 = map(float, parts[:4])
                        label = " ".join(parts[4:])
                        
                        pali_objects.append({
                            "bbox": [x1, y1, x2, y2],
                            "confidence": 0.7,  # Default confidence for PaLI
                            "label": label
                        })
                        
                        detected_artifacts.append({
                            "bbox": [x1, y1, x2, y2],
                            "confidence": 0.7,
                            "label": label,
                            "source": "PaLI"
                        })
                        
                        reflection_text += f"PaLI detected object: {label}. "
                        
                except Exception as e:
                    print(f"Warning: Failed to parse PaLI bbox data: {e}")
            
            # Calculate derived scores based on detection data
            artifact_score = min(len(detected_artifacts) * 0.2, 1.0)  # Simple artifact scoring
            misalignment_score = 0.8 if misalignment_detected else 0.1
            overall_quality = max(0.2, 1.0 - artifact_score - (misalignment_score * 0.5))
            
            # Add artifact score analysis
            if artifact_score > 0.3:
                reflection_text += f"High artifact presence (score: {artifact_score:.3f}). "
                reflection_text += "Generating artifact heatmap for targeted correction. "
                
            if misalignment_score > 0.3:
                reflection_text += f"Significant prompt misalignment (score: {misalignment_score:.3f}). "
                reflection_text += "Analyzing correspondence between prompt elements and visual content. "
                
            if overall_quality < 0.6:
                reflection_text += f"Below quality threshold (score: {overall_quality:.3f}). "
                reflection_text += "Identifying specific regions requiring enhancement. "
                
            # Add MINT-specific reflection components
            reflection_text += "Performing multi-modal chain of thought analysis. "
            reflection_text += "Self-reflection on generation accuracy and semantic alignment. "
            if detected_artifacts:
                reflection_text += f"Preparing {len(detected_artifacts)} bounding box corrections for artifact removal."
            else:
                reflection_text += "No major artifacts detected, focusing on quality enhancement."
            
            # Return MINT-formatted reflection data
            return {
                "image_id": f"seetrue_{sample_id}",
                "prompt": prompt,
                "reflection": reflection_text,
                "artifact_score": artifact_score,
                "misalignment_score": misalignment_score,
                "overall_quality": overall_quality,
                "bbox_artifacts": detected_artifacts,
                "misalignment_detected": misalignment_detected,
                "artifact_confidence": max(artifact_score, misalignment_score),
                "grounding_dino_count": len(grounding_dino_objects),
                "pali_detection_count": len(pali_objects),
                # MINT-specific fields for reflection task
                "requires_artifact_correction": artifact_score > 0.3 or len(detected_artifacts) > 0,
                "requires_alignment_correction": misalignment_detected or misalignment_score > 0.3,
                "requires_quality_improvement": overall_quality < 0.6,
                "seetrue_enhanced": True
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping SeeTRUE sample {sample_id}: Failed to process - {e}")
            return None

    def _process_brush_data(self, brush_raw_dir, processed_data_dir):
        """Process BrushData from direct wget downloads (limited to 20GB)"""
        correction_samples = []
        
        print("üîÑ Processing BrushData from direct wget downloads...")
        
        # Check if we have direct downloads
        downloaded_tars_file = brush_raw_dir / "downloaded_tars.json"
        tars_dir = brush_raw_dir / "tars"
        
        if downloaded_tars_file.exists():
            try:
                with open(downloaded_tars_file, 'r') as f:
                    tar_file_paths = [Path(p) for p in json.load(f)]
                print(f"üìÇ Found {len(tar_file_paths)} downloaded TAR files for BrushData")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not read downloaded TAR files list: {e}")
                tar_file_paths = []
        else:
            # Fallback: look for TAR files in directory
            tar_file_paths = list(tars_dir.glob("*.tar")) if tars_dir.exists() else []
            print(f"üìÇ Found {len(tar_file_paths)} TAR files in {tars_dir}")
        
        if not tar_file_paths:
            raise FileNotFoundError("No BrushData TAR files found")
        if wds is None:
            raise ImportError("Webdataset library not available. Cannot process BrushData TAR files.")
        else:
            processed_sample_count = 0
            MAX_SAMPLES = 2000  # Process more samples since we limited download size
            
            for i, tar_file_path in enumerate(tar_file_paths):
                if processed_sample_count >= MAX_SAMPLES:
                    print(f"üõë Reached {MAX_SAMPLES} sample limit, stopping processing")
                    break
                    
                print(f"üîÑ Processing BrushData TAR file ({i+1}/{len(tar_file_paths)}): {tar_file_path.name}")
                try:
                    # BrushData TAR files already contain processed data with 'image', 'caption', etc. keys
                    # Don't use .decode() or .rename() as the data is already structured
                    dataset_shard = wds.WebDataset(str(tar_file_path))
                    
                    for sample_idx, sample in enumerate(dataset_shard):
                        if processed_sample_count >= MAX_SAMPLES:
                            break
                        
                        # Debug: Print comprehensive sample structure for first few samples
                        if sample_idx < 3:  # Debug first 3 samples from first TAR file
                            print(f"üîç DEBUG Sample {sample_idx} structure:")
                            print(f"   Available keys: {list(sample.keys())}")
                            for key, value in sample.items():
                                if isinstance(value, bytes):
                                    # Try to decode bytes to understand content
                                    try:
                                        decoded_str = value.decode('utf-8')
                                        print(f"   {key}: bytes‚Üístring ('{decoded_str[:100]}...' if len > 100)")
                                    except:
                                        print(f"   {key}: bytes (length: {len(value)}, non-text)")
                                elif isinstance(value, str):
                                    print(f"   {key}: string ('{value[:100]}...' if len > 100)")
                                elif hasattr(value, 'shape'):  # numpy array or tensor
                                    print(f"   {key}: array/tensor (shape: {value.shape}, dtype: {getattr(value, 'dtype', 'unknown')})")
                                elif hasattr(value, 'mode'):  # PIL Image
                                    print(f"   {key}: PIL Image (mode: {value.mode}, size: {value.size})")
                                else:
                                    print(f"   {key}: {type(value)} - {str(value)[:100]}...")
                        
                        # Handle image data - it might be raw bytes that need to be converted to PIL
                        image_pil = None
                        mask_pil = None
                        
                        if "image" in sample:
                            image_data = sample["image"]
                            if isinstance(image_data, bytes):
                                try:
                                    from io import BytesIO
                                    image_pil = Image.open(BytesIO(image_data))
                                except Exception as e:
                                    raise ValueError(f"Could not decode image data: {e}")
                            elif hasattr(image_data, 'mode'):  # Already a PIL image
                                image_pil = image_data
                        
                        # Handle mask/segmentation data - BrushData uses RLE JSON format
                        mask_pil = None
                        segmentation_data = None
                        if "segmentation" in sample:
                            mask_data = sample["segmentation"]
                            if isinstance(mask_data, bytes):
                                try:
                                    # First try: decode as JSON (RLE format)
                                    seg_text = mask_data.decode('utf-8')
                                    segmentation_data = json.loads(seg_text)
                                    
                                    if sample_idx < 3:  # Debug output
                                        print(f"   segmentation: RLE JSON with {len(segmentation_data.get('mask', []))} masks")
                                    
                                    # Successfully parsed RLE data - we'll use this for mask generation
                                    # For now, mark as having segmentation data (we could convert RLE to PIL later if needed)
                                    has_segmentation_data = True
                                    
                                except json.JSONDecodeError:
                                    try:
                                        # Second try: direct image decoding (fallback)
                                        from io import BytesIO
                                        mask_pil = Image.open(BytesIO(mask_data))
                                        has_segmentation_data = True
                                        if sample_idx < 3:
                                            print(f"   segmentation: decoded as PIL image")
                                    except Exception as e:
                                        if sample_idx < 3:
                                            print(f"   segmentation: failed to decode - {e}")
                                        has_segmentation_data = False
                                except Exception as e:
                                    if sample_idx < 3:
                                        print(f"   segmentation: failed to parse as JSON - {e}")
                                    has_segmentation_data = False
                            elif hasattr(mask_data, 'mode'):  # Already a PIL image
                                mask_pil = mask_data
                                has_segmentation_data = True
                            elif isinstance(mask_data, dict):  # Already parsed JSON
                                segmentation_data = mask_data
                                has_segmentation_data = True
                                if sample_idx < 3:
                                    print(f"   segmentation: pre-parsed RLE JSON with {len(mask_data.get('mask', []))} masks")
                            else:
                                has_segmentation_data = False
                        
                        if image_pil is None:
                            raise ValueError(f"Sample {sample_idx} does not contain valid image data")
                        
                        # Use either PIL mask or RLE segmentation data to determine if we have mask info
                        has_actual_mask = mask_pil is not None or segmentation_data is not None
                        
                        caption_text = ""
                        task_type_content = "inpainting" 

                        if "caption" in sample:
                            cap_data = sample["caption"]
                            if isinstance(cap_data, (bytes, str)):
                                caption_text = cap_data.decode('utf-8').strip() if isinstance(cap_data, bytes) else str(cap_data).strip()
                            elif isinstance(cap_data, dict): 
                                caption_text = cap_data.get("caption", cap_data.get("text",""))
                                task_type_content = cap_data.get("task_type", task_type_content)
                        
                        has_actual_mask = mask_pil is not None or segmentation_data is not None
                        image_id_str = f"brush_{tar_file_path.stem}_{sample.get('__key__', sample_idx)}"

                        correction_text_gen = self._generate_mint_correction_text(
                            has_mask=has_actual_mask,
                            task_type=task_type_content,
                            caption=caption_text
                        )
                        correction_samples.append({
                            "image_id": image_id_str,
                            "correction": correction_text_gen,
                            "has_mask": has_actual_mask,
                            "task_type": task_type_content,
                            "caption": caption_text,
                            "segmentation_rle": segmentation_data,  # Store RLE data for potential future use
                            "mask_pil": None,  # Don't store PIL objects in JSON
                            "has_image": True,  # Flag to indicate image availability
                            "tar_file_path": str(tar_file_path),  # Store TAR file path for image reloading
                            "sample_key": sample.get('__key__', sample_idx)  # Store sample key for identification
                        })
                        processed_sample_count += 1
                        
                        if processed_sample_count % 100 == 0:
                            print(f"üìà Processed {processed_sample_count} BrushData samples...")
                            
                except Exception as e_tar:
                    raise RuntimeError(f"Error processing TAR file {tar_file_path}: {e_tar}") 
                    
            print(f"‚úÖ Processed {processed_sample_count} samples from {len(tar_file_paths)} BrushData TAR files")

        if not correction_samples:
            raise RuntimeError("No correction data was successfully processed from BrushData TAR files")

        with open(processed_data_dir / "correction_data.json", 'w') as f:
            json.dump(correction_samples, f)
        self.brush_correction_samples = correction_samples

    def _reload_image_from_tar(self, correction_sample):
        """Reload PIL image from TAR file using stored metadata"""
        tar_file_path = correction_sample.get("tar_file_path")
        sample_key = correction_sample.get("sample_key")
        
        if not tar_file_path or sample_key is None:
            missing_fields = []
            if not tar_file_path:
                missing_fields.append("tar_file_path")
            if sample_key is None:
                missing_fields.append("sample_key")
            print(f"‚ö†Ô∏è Warning: Cannot reload image - missing required fields: {', '.join(missing_fields)}")
            return None
            
        try:
            # Open the specific TAR file and find the sample
            dataset_shard = wds.WebDataset(tar_file_path)
            
            for sample in dataset_shard:
                if sample.get('__key__') == sample_key:
                    # Found the matching sample, extract image
                    if "image" in sample:
                        image_data = sample["image"]
                        if isinstance(image_data, bytes):
                            from io import BytesIO
                            return Image.open(BytesIO(image_data))
                        elif hasattr(image_data, 'mode'):  # Already a PIL image
                            return image_data
                    break
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not reload image from TAR file {tar_file_path}: {e}")
            
        print(f"‚ö†Ô∏è Warning: Failed to find or load image with key '{sample_key}' from TAR file {tar_file_path}")
        return None

    def _generate_mint_correction_text(self, has_mask=True, task_type="inpainting", caption="", mask_coverage=0.0):
        if not has_mask and mask_coverage == 0.0:
            raise ValueError("No mask data available and no mask coverage provided - cannot generate correction text")
        
        correction_text = ""
        if task_type == "segmentation_inpainting" and has_mask:
            correction_text = "Correction: Segmentation-based inpainting with BrushNet methodology. "
            correction_text += f"Caption guidance: '{caption}' " if caption else ""
            correction_text += "Precise object-aware segmentation masks target specific regions. "
        elif task_type == "random_inpainting":
            correction_text = "Correction: Random mask inpainting with authentic BrushNet generation. "
            correction_text += f"Caption guidance: '{caption}' " if caption else ""
            if mask_coverage > 0: correction_text += f"Generated mask coverage: {mask_coverage:.2%}. "
        else:
            correction_text = "Correction: Targeted artifact correction using BrushNet inpainting. "
            correction_text += f"Caption guidance: '{caption}' " if caption else ""
        return self._enhance_correction_with_brushnet(correction_text, has_mask, mask_coverage)

    def _enhance_correction_with_brushnet(self, correction_text, has_mask=True, coverage=0.0):
        if has_mask:
            correction_text += f" BrushNet segmentation approach with {coverage:.1%} coverage. "
        else:
            correction_text += f"BrushNet random brush generation with {coverage:.1%} coverage. "
        correction_text += "Authentic TencentARC/BrushNet methodology applied."
        return correction_text

    def _generate_random_mask(self, height=512, width=512):
        if Image is None or ImageDraw is None: 
            raise ImportError("PIL (Image and ImageDraw) is required for random mask generation")
        
        mask_array = self._brushnet_random_mask_gen(height, width)
        if mask_array is None: 
            raise RuntimeError("Failed to generate random mask array")
        
        mask = Image.fromarray((mask_array * 255).astype(np.uint8), 'L')
        coverage = np.sum(mask_array > 0) / (height * width)
        return mask, coverage

    def _brushnet_random_brush_gen(self, max_tries, h, w, min_num_vertex=4, max_num_vertex=8,
                                   mean_angle=2*math.pi/5, angle_range=2*math.pi/15,
                                   min_width=12, max_width=40):
        """Generate random brush mask following authentic BrushNet methodology from TencentARC/BrushNet"""
        if Image is None or ImageDraw is None: 
            raise ImportError("PIL (Image and ImageDraw) is required for brush mask generation")
        
        H, W = h, w
        average_radius = math.sqrt(H*H + W*W) / 8
        mask = Image.new('L', (W, H), 0)
        
        # Generate random number of brush strokes (authentic BrushNet uses np.random.randint)
        for _ in range(np.random.randint(1, max_tries)):
            num_vertex = np.random.randint(min_num_vertex, max_num_vertex)
            angle_min = mean_angle - np.random.uniform(0, angle_range)
            angle_max = mean_angle + np.random.uniform(0, angle_range)
            
            # Generate angles for each vertex (authentic BrushNet approach)
            angles = []
            vertex = []
            for i in range(num_vertex):
                if i % 2 == 0:
                    angles.append(2*math.pi - np.random.uniform(angle_min, angle_max))
                else:
                    angles.append(np.random.uniform(angle_min, angle_max))
            
            # Start with random position
            vertex.append((int(np.random.randint(0, W)), int(np.random.randint(0, H))))
            
            # Generate subsequent vertices
            for i in range(num_vertex):
                r = np.clip(
                    np.random.normal(loc=average_radius, scale=average_radius//2),
                    0, 2*average_radius)
                new_x = np.clip(vertex[-1][0] + r * math.cos(angles[i]), 0, W)
                new_y = np.clip(vertex[-1][1] + r * math.sin(angles[i]), 0, H)
                vertex.append((int(new_x), int(new_y)))

            # Draw the brush stroke
            draw = ImageDraw.Draw(mask)
            width = int(np.random.uniform(min_width, max_width))
            draw.line(vertex, fill=1, width=width)
            
            # Draw ellipses at each vertex for brush tip effect (authentic BrushNet approach)
            for v in vertex:
                draw.ellipse((v[0] - width//2,
                            v[1] - width//2,
                            v[0] + width//2,
                            v[1] + width//2),
                            fill=1)
        
        # Apply random transformations (authentic BrushNet approach)
        if np.random.random() > 0.5:
            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)
        if np.random.random() > 0.5:
            mask = mask.transpose(Image.FLIP_TOP_BOTTOM)
        
        mask = np.asarray(mask, np.uint8)
        if np.random.random() > 0.5:
            mask = np.flip(mask, 0)
        if np.random.random() > 0.5:
            mask = np.flip(mask, 1)
        return mask

    def _brushnet_random_mask_gen(self, h, w):
        """Generate random mask following authentic BrushNet methodology"""
        mask = np.ones((h, w), np.uint8)
        # hole denoted as 0, reserved as 1 (authentic BrushNet approach)
        # Use np.logical_and as in authentic implementation
        brush_mask = self._brushnet_random_brush_gen(4, h, w)
        mask = np.logical_and(mask, 1 - brush_mask).astype(np.float32)
        return mask


    def _generate_mcot_examples(self, processed_data_dir):
        planning_samples = getattr(self, "actplan_planning_samples_with_images", [])
        richhf_reflection_samples = getattr(self, "richhf_reflection_samples", [])
        seetrue_reflection_samples = getattr(self, "seetrue_reflection_samples", [])
        correction_samples = getattr(self, "brush_correction_samples", [])

        if not planning_samples: 
            try:
                with open(processed_data_dir / "planning_data.json", 'r') as f: planning_samples_json = json.load(f)
                planning_samples = []
                for ps_json in planning_samples_json:
                    ps_json["image"] = None 
                    planning_samples.append(ps_json)
                print(f"üìÑ Loaded {len(planning_samples)} planning samples from JSON")
            except FileNotFoundError: 
                print(f"‚ùå ERROR: planning_data.json not found in {processed_data_dir}")
                raise FileNotFoundError("planning_data.json not found - required for MCoT examples generation")
            except Exception as e:
                print(f"‚ùå ERROR: Failed to load planning_data.json: {e}")
                raise
        
        if not richhf_reflection_samples:
            try:
                with open(processed_data_dir / "richhf_reflection_data.json", 'r') as f: richhf_reflection_samples = json.load(f)
                print(f"üìÑ Loaded {len(richhf_reflection_samples)} RichHF reflection samples from JSON")
            except FileNotFoundError: 
                print(f"‚ùå ERROR: richhf_reflection_data.json not found in {processed_data_dir}")
                raise FileNotFoundError("richhf_reflection_data.json not found - required for MCoT examples generation")
            except Exception as e:
                print(f"‚ùå ERROR: Failed to load richhf_reflection_data.json: {e}")
                raise
        
        if not seetrue_reflection_samples:
            try:
                with open(processed_data_dir / "seetrue_reflection_data.json", 'r') as f: seetrue_reflection_samples = json.load(f)
                print(f"üìÑ Loaded {len(seetrue_reflection_samples)} SeeTRUE reflection samples from JSON")
            except FileNotFoundError: 
                print(f"‚ùå ERROR: seetrue_reflection_data.json not found in {processed_data_dir}")
                raise FileNotFoundError("seetrue_reflection_data.json not found - required for MCoT examples generation")
            except Exception as e:
                print(f"‚ùå ERROR: Failed to load seetrue_reflection_data.json: {e}")
                raise

        if not correction_samples:
            try:
                with open(processed_data_dir / "correction_data.json", 'r') as f: correction_samples = json.load(f)
                print(f"üìÑ Loaded {len(correction_samples)} BrushData correction samples from JSON")
            except FileNotFoundError: 
                print(f"‚ùå ERROR: correction_data.json not found in {processed_data_dir}")
                raise FileNotFoundError("correction_data.json not found - required for MCoT examples generation")
            except Exception as e:
                print(f"‚ùå ERROR: Failed to load correction_data.json: {e}")
                raise

        mcot_examples = []
        num_planning = len(planning_samples)
        num_richhf_reflection = len(richhf_reflection_samples) if richhf_reflection_samples else 0
        num_seetrue_reflection = len(seetrue_reflection_samples) if seetrue_reflection_samples else 0
        num_correction = len(correction_samples) if correction_samples else 0

        print(f"üìä MCoT Data Summary:")
        print(f"   üìù Planning samples: {num_planning}")
        print(f"   ü§î RichHF reflection samples: {num_richhf_reflection}")
        print(f"   üëÅÔ∏è  SeeTRUE reflection samples: {num_seetrue_reflection}")
        print(f"   üé® BrushData correction samples: {num_correction}")

        if num_planning == 0: 
            print(f"‚ùå ERROR: No planning samples available to generate MCoT examples")
            raise RuntimeError("No planning samples available to generate MCoT examples") 

        for i in range(num_planning):
            planning_sample = planning_samples[i]
            image_id = planning_sample.get("image_id", f"mcot_{i}")
            
            # For MCoT examples, we need actual images. Since planning samples (ActPlan) are text-only,
            # we skip MCoT generation when no images are available (fail fast approach)
            if not hasattr(self, 'brush_correction_samples') or not self.brush_correction_samples:
                print(f"‚ùå ERROR: No BrushData correction samples with images available for MCoT generation")
                print(f"   This means either BrushData failed to download or process correctly")
                break
                
            # Use image from correction samples that have actual PIL images  
            correction_s = correction_samples[i % num_correction] if num_correction > 0 else {}
            
            # Reload PIL image from TAR file using stored metadata
            pil_image_obj = None
            if correction_s.get("has_image", False):
                try:
                    pil_image_obj = self._reload_image_from_tar(correction_s)
                    if pil_image_obj is None:
                        print(f"‚ö†Ô∏è Warning: Failed to reload image from TAR file for sample {i}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Warning: Error reloading image for sample {i}: {e}")
                    pil_image_obj = None
            
            # Skip examples without valid PIL images (fail fast approach)
            if not isinstance(pil_image_obj, Image.Image):
                print(f"‚ö†Ô∏è Warning: Skipping MCoT example {i} - no valid PIL image (type: {type(pil_image_obj)})")
                continue

            # MINT paper uses both RichHF-18K AND manually annotated images for reflection
            # We use SeeTrue as replacement for the 5,000 manually annotated MINT images
            # Combine both datasets to fully replicate MINT reflection methodology
            reflection_s = {}
            if num_richhf_reflection > 0 and num_seetrue_reflection > 0:
                # Use both sources - randomly choose between RichHF and SeeTrue
                if i % 2 == 0:
                    reflection_s = richhf_reflection_samples[i % num_richhf_reflection]
                else:
                    reflection_s = seetrue_reflection_samples[i % num_seetrue_reflection]
            elif num_richhf_reflection > 0:
                reflection_s = richhf_reflection_samples[i % num_richhf_reflection]
            elif num_seetrue_reflection > 0:
                reflection_s = seetrue_reflection_samples[i % num_seetrue_reflection]

            # For acting step, we'll use planning prompt since RichHF is now used for reflection
            acting_prompt = planning_sample.get("prompt", f"Generate image for {image_id}")
            acting_text = f"Generating image based on prompt: {acting_prompt}"

            final_response = f"Complete MCoT process for {image_id}. Enhanced through planning, acting, reflection, and correction."
            mcot_example = {
                "image_id": image_id,
                "prompt": acting_prompt,
                "planning": planning_sample.get("planning", "Planning data N/A."),
                "acting": acting_text,
                "reflection": reflection_s.get("reflection", "Reflection data N/A."),
                "correction": correction_s.get("correction", "Correction data N/A."),
                "final_response": final_response,
                "image_obj_pil": pil_image_obj,
                "image_mode_from_planning": planning_sample.get("image_mode"), 
                "image_size_from_planning": planning_sample.get("image_size")   
            }
            mcot_examples.append(mcot_example)

        print(f"üéØ Generated {len(mcot_examples)} total MCoT examples")
        
        random.shuffle(mcot_examples)
        split_idx = int(0.9 * len(mcot_examples))
        train_examples = mcot_examples[:split_idx]
        val_examples = mcot_examples[split_idx:]

        print(f"üìÇ Creating train split with {len(train_examples)} examples...")
        self._save_mcot_examples_split(train_examples, processed_data_dir / "train")
        
        print(f"üìÇ Creating val split with {len(val_examples)} examples...")
        self._save_mcot_examples_split(val_examples, processed_data_dir / "val")
        
        print(f"‚úÖ MCoT examples saved successfully!")
        print(f"   üìÅ Train directory: {processed_data_dir / 'train'}")
        print(f"   üìÅ Val directory: {processed_data_dir / 'val'}")

    def _save_mcot_examples_split(self, examples, output_dir_split):
        print(f"üìÅ Creating split directory: {output_dir_split}")
        output_dir_split.mkdir(parents=True, exist_ok=True)
        
        if not output_dir_split.exists():
            print(f"‚ùå ERROR: Failed to create directory {output_dir_split}")
            raise RuntimeError(f"Could not create directory {output_dir_split}")
        
        print(f"‚úÖ Directory created successfully: {output_dir_split}")
        print(f"üìÑ Saving {len(examples)} examples to {output_dir_split}")
        
        for i, example in enumerate(examples):
            image_id_val = example.get('image_id', f"unknown_{i}")
            safe_image_id_val = image_id_val.replace('/', '_').replace('\\', '_')
            example_data_dir = output_dir_split / f"example_{safe_image_id_val}"
            
            try:
                example_data_dir.mkdir(parents=True, exist_ok=True)
                
                annotations_to_save = {k: v for k, v in example.items() if k not in ["image_obj_pil", "image_mode_from_planning", "image_size_from_planning"]}
                with open(example_data_dir / "mcot_annotations.json", "w") as f:
                    json.dump(annotations_to_save, f, indent=2)

                pil_image = example.get("image_obj_pil")
                image_file_path = example_data_dir / "image.jpg"
                
                if isinstance(pil_image, Image.Image):
                    try:
                        pil_image.convert("RGB").save(image_file_path, "JPEG")
                    except Exception as e:
                        print(f"‚ùå ERROR: Could not save PIL image for {image_id_val}: {e}")
                        raise RuntimeError(f"Could not save PIL image for {image_id_val}: {e}")
                else:
                    print(f"‚ùå ERROR: No valid PIL image provided for {image_id_val} (type: {type(pil_image)})")
                    raise ValueError(f"No valid PIL image provided for {image_id_val}")
                    
                if (i + 1) % 10 == 0:
                    print(f"   ‚úÖ Saved {i + 1}/{len(examples)} examples")
                    
            except Exception as e:
                print(f"‚ùå ERROR: Failed to save example {image_id_val}: {e}")
                raise
        
        print(f"‚úÖ Successfully saved all {len(examples)} examples to {output_dir_split}")

    def _generate_examples(self, data_dir):
        data_path = Path(data_dir)
        example_dirs = [d for d in data_path.iterdir() if d.is_dir() and (d / "mcot_annotations.json").exists()]

        for i, example_dir_path in enumerate(example_dirs):
            mcot_file = example_dir_path / "mcot_annotations.json"
            image_file = example_dir_path / "image.jpg"

            if not image_file.exists(): 
                raise FileNotFoundError(f"Image file missing for {example_dir_path}")
            
            try:
                with open(mcot_file, "r") as f:
                    annotations = json.load(f)
            except json.JSONDecodeError:
                raise ValueError(f"Could not decode annotations for {example_dir_path}")
            
            yield i, {
                "image_id": annotations.get("image_id", example_dir_path.name.replace("example_","")),
                "image": str(image_file),
                "prompt": annotations.get("prompt"),
                "planning": annotations.get("planning"),
                "acting": annotations.get("acting"),
                "reflection": annotations.get("reflection"),
                "correction": annotations.get("correction"),
                "final_response": annotations.get("final_response")
            }


# Main execution block for direct script usage
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Download and process MCoT datasets")
    parser.add_argument("--output_dir", type=str, default="./mcot_dataset_output", 
                       help="Output directory for processed dataset")
    parser.add_argument("--cache_dir", type=str, default="./mcot_dataset_cache",
                       help="Cache directory for downloaded files")
    parser.add_argument("--num_proc", type=int, default=4,
                       help="Number of processes for dataset processing")
    
    args = parser.parse_args()
    
    print("Starting MCoT dataset download and processing...")
    print(f"Output directory: {args.output_dir}")
    print(f"Cache directory: {args.cache_dir}")
    
    # Process data directly without using HuggingFace datasets builder framework
    try:
        # Create directory structure
        output_path = Path(args.output_dir)
        cache_path = Path(args.cache_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        cache_path.mkdir(parents=True, exist_ok=True)
        
        # Create a simple download manager-like object
        class SimpleDownloadManager:
            def __init__(self, cache_dir):
                self.cache_dir = cache_dir
                self.manual_dir = cache_dir
                
            def download_and_extract(self, url):
                """Robust download and extract implementation using wget"""
                import subprocess
                import zipfile
                filename = url.split('/')[-1]
                local_path = Path(self.cache_dir) / filename
                
                if not local_path.exists():
                    print(f"Downloading {url} using wget...")
                    try:
                        # Use wget with robust options:
                        # -c: continue partial downloads
                        # --timeout=0: no connection timeout (wait indefinitely)
                        # --tries=3: retry up to 3 times
                        # --progress=bar: show progress bar
                        wget_cmd = [
                            'wget',
                            '-c',                    # Continue partial downloads
                            '--timeout=0',           # No connection timeout (wait indefinitely)
                            '--tries=3',             # Reasonable number of retries
                            '--wait=5',              # Wait 5 seconds between retries
                            '--progress=bar:force',  # Force progress bar display
                            '--show-progress',       # Show download progress
                            '--user-agent=Mozilla/5.0 (compatible; Research Bot)', # Add user agent
                            '-O', str(local_path),   # Output to specific file
                            url
                        ]
                        
                        print(f"Running: {' '.join(wget_cmd)}")
                        result = subprocess.run(
                            wget_cmd, 
                            check=True, 
                            capture_output=False,  # Show wget output directly
                            text=True,
                            timeout=None  # No subprocess timeout - allow downloads to complete
                        )
                        print(f"‚úÖ Download completed: {local_path}")
                        
                    except subprocess.CalledProcessError as e:
                        print(f"‚ùå wget failed with exit code {e.returncode}: {url}")
                        if local_path.exists():
                            local_path.unlink()  # Remove partial/corrupted file
                        raise
                    except FileNotFoundError:
                        print("‚ùå wget not found. Please install wget or use alternative download method.")
                        raise
                else:
                    print(f"üìÅ File already exists: {local_path}")
                
                # Extract if it's a zip file
                if filename.endswith('.zip'):
                    extract_dir = local_path.parent / filename.replace('.zip', '')
                    if not extract_dir.exists():
                        print(f"üì¶ Extracting {local_path}...")
                        try:
                            with zipfile.ZipFile(local_path, 'r') as zip_ref:
                                zip_ref.extractall(extract_dir)
                            print(f"‚úÖ Extraction completed: {extract_dir}")
                        except zipfile.BadZipFile:
                            print(f"‚ùå Corrupted zip file: {local_path}")
                            local_path.unlink()  # Remove corrupted file
                            raise
                    else:
                        print(f"üìÅ Already extracted: {extract_dir}")
                    return str(extract_dir)
                    
                return str(local_path)
                
            def download_config(self, *args, **kwargs):
                """Placeholder for download_config - not needed for our direct processing"""
                pass
        
        # Create dataset builder and process data
        dataset_builder = MCoTWgetDataset()
        dl_manager = SimpleDownloadManager(str(cache_path))
        
        # Call the split generators directly to process the data
        print("Processing MCoT dataset components...")
        split_generators = dataset_builder._split_generators(dl_manager)
        
        print("Dataset processing completed successfully!")
        print(f"Processed data saved to: {cache_path}")
        print("You can now use the MCoT PyTorch dataset classes to load this data for training.")
        
    except Exception as e:
        print(f"Error during dataset processing: {e}")
        import traceback
        print("Full error traceback:")
        traceback.print_exc()
        print("Please check the error messages above for specific issues.")
    
    print("MCoT dataset processing completed!")
