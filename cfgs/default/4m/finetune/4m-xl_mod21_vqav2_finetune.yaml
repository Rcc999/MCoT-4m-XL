run_name: vqav2_finetune_4m-xl_mod21

num_input_tokens: 256
num_target_tokens: 64 # Adjusted for VQA answer length
loss_type: mod

model: fm_xlarge_24e_24d_swiglu_nobias
patch_size: 16
input_size: 224
dtype: bfloat16
tokenizer_path: "fourm/utils/tokenizer/trained/text_tokenizer_4m_wordpiece_30k.json"

finetune: 'https://huggingface.co/EPFL-VILAB/4M-21_XL/resolve/main/model.safetensors' # IMPORTANT: Using the Hugging Face URL for the 4m21xl model checkpoint

epochs: 10 # Adjusted from -1
total_tokens: -1 # Adjusted from 5
opt: adamw
blr: 0.00002
min_blr: 0.
warmup_epochs: 1 # Adjusted from -1
warmup_tokens: -1 # Adjusted from 0.4
batch_size: 32 #32
clip_grad: 3.0
use_act_checkpoint: True
skip_nan_grad: True

data_config: "cfgs/default/4m/data/vqav2/vqav2.yaml" 
eval_freq: 1
fixed_eval: True
epoch_size: 100_000

save_ckpt_freq: 1
output_dir: 'output/vqav2_finetune_4m-xl_mod21'

# Wandb 
log_wandb: True
wandb_project: '4m-vqav2-finetune'
wandb_entity: 'as8148-epfl'