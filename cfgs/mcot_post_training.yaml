# Configuration file for MCOT Post-training (Stage 2)
# This configuration starts from the VQA-tuned checkpoint from Stage 1
# and implements the MCOT multi-task learning setup.

# Model and base parameters
model: "fm_xl_32e_32d"  # 4M-21_XL model
patch_size: 16
input_size: 224
dtype: "bfloat16"
num_input_tokens: 1024
num_target_tokens: 512  # Increased for handling different modalities

# Fine-tuning parameters
finetune: "./output/mcot_vqa_finetune/checkpoint-best.pth"  # Path to VQA-tuned checkpoint
loss_type: "token"  # For multi-task sequence-to-sequence learning

# Optimizer parameters
batch_size: 16  # Smaller due to multi-task complexity
epochs: 10
opt: "adamw"
weight_decay: 0.05
blr: 1e-5  # Lower learning rate for post-training
min_blr: 0.0
clip_grad: 1.0
warmup_epochs: 1

# Scheduler
scheduler: "cosine"

# Dataset parameters
data_config: "cfgs/data_mcot.yaml"  # Will create this file
text_tokenizer_path: "fourm/utils/tokenizer/trained/text_tokenizer_4m_wordpiece_30k.json"

# Evaluation parameters
eval_freq: 1
fixed_eval: true
fixed_eval_batch_size: 8

# Output directory
output_dir: "./output/mcot_post_training"

# Misc
num_workers: 8
pin_mem: true 